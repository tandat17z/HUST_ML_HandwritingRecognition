{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\HUST\\20232\\ML\\Project_OCR\\HandwritingRecognition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tandat17z\\anaconda3\\envs\\env_tandat17z\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.MyCrnn import MyCRNN\n",
    "from dataset import DatasetImg\n",
    "from utils.utils import strLabelConverter\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetImg('data/4/img', 'data/4/label', 512, 32)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "with open(os.path.join('data/mychar.txt'), 'r', encoding='utf-8') as f:\n",
    "    alphabet = f.read().rstrip()\n",
    "# print(alphabet)\n",
    "converter = strLabelConverter(alphabet)\n",
    "print(converter.numClass)\n",
    "# --------------------- Create Model ---------------------------------\n",
    "model = MyCRNN(converter.numClass, 100, 0.1).to('cpu')\n",
    "# print(f\"Model structure: {model}\\n\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "criterion = torch.nn.CTCLoss().to('cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "    (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu7): ReLU(inplace=True)\n",
       "    (pooling4): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (biLSTM1): LSTM(512, 100, batch_first=True, bidirectional=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=200, out_features=154, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 35,  76,  40,   1,  41,  28,  52,   1,  39,  64,   1,  47,  35, 128,\n",
       "           1,  40,  94,  52,  13,   1,  35,  76,  40,   1,  41,  28,  52,   1,\n",
       "          39,  64,   1,  47,  35, 128,   1,  17], dtype=torch.int32),\n",
       " tensor([19, 16], dtype=torch.int32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.encode(['hôm nay là thứ mấy.', 'hôm nay là thứ 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tqdm(iter(train_dataloader), total=len(train_dataloader), desc='Epoch {}'.format(1))\n",
    "for batch_idx, (imgs, labels) in enumerate(t):\n",
    "    targets, target_lenghts = converter.encode(labels)\n",
    "    targets = targets.to('cpu')\n",
    "    target_lenghts = target_lenghts.to('cpu')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds = model(imgs)\n",
    "    \n",
    "    b, l, c = preds.shape\n",
    "    preds_ = preds.permute(1, 0, 2).to('cpu')\n",
    "    preds_lengths = torch.full(size=(b,), fill_value=l, dtype=torch.long).to('cpu')\n",
    "\n",
    "    loss = criterion(preds_.log_softmax(2), targets, preds_lengths, target_lenghts) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "    assert (not torch.isnan(loss) and not torch.isinf(loss)), \"Loss value is NaN or Inf\"\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # avg_loss += loss.detach().item()\n",
    "\n",
    "    # _, enc_preds = preds.max(2)\n",
    "    # sim_preds = self.converter.decode(enc_preds.view(-1), preds_lengths, raw = False)\n",
    "    # avg_levenshtein_loss += self.converter.Levenshtein_loss(sim_preds, labels)\n",
    "\n",
    "        return avg_loss/len(self.train_dataloader), avg_levenshtein_loss/self.train_dataloader.sampler.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chuc vo dich Bundesliga thu 3 lien tiep den vao nam 2001',)\n",
      "tensor([30, 35, 48, 30,  1, 49, 42,  1, 31, 36, 30, 35,  1, 29, 48, 41, 31, 32,\n",
      "        46, 39, 36, 34, 28,  1, 47, 35, 48,  1, 18,  1, 39, 36, 32, 41,  1, 47,\n",
      "        36, 32, 43,  1, 31, 32, 41,  1, 49, 28, 42,  1, 41, 28, 40,  1, 17, 15,\n",
      "        15, 16], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.2001, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(iter(train_dataloader))\n",
    "imgs = imgs.to('cpu')\n",
    "\n",
    "print(labels)\n",
    "targets, target_lenghts = converter.encode(labels)\n",
    "print(targets)\n",
    "targets = targets.to('cpu')\n",
    "target_lenghts = target_lenghts.to('cpu')\n",
    "\n",
    "preds_ = model(imgs)\n",
    "\n",
    "b, l, c = preds_.shape\n",
    "preds = preds_.permute(1, 0, 2).to('cpu')\n",
    "preds_lengths = torch.full(size=(b,), fill_value=l, dtype=torch.long).to('cpu')\n",
    "loss = criterion(preds.log_softmax(2), targets, preds_lengths, target_lenghts) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 41,  34,  91, 122,  36,   1,  85,  91, 125,  30,   1,  43,  35,  65,\n",
       "          47,   1,  35,  28,  36,   1,  43,  35,  95,  41,   1,  30,  90,  40,\n",
       "           1,  14,   1,  41,  34,  64,  52,  11,   1,  30,  93,   1,  46,  48,\n",
       "          94,  47,   1,  47,  45,  91,  28,   1,  49,  64,   1,  30,  35,  36,\n",
       "         108,  48,   1,  31, 117,  41,   1,  40, 120,  47,  13,   1,  30,  48,\n",
       "         120,  30,   1,  46, 116,  41,  34,   1,  38,  69,  42],\n",
       "        dtype=torch.int32),\n",
       " tensor([81], dtype=torch.int32))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc, len = converter.encode(labels)\n",
    "enc, len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "torch.set_printoptions(threshold=float('inf'))\n",
    "np.set_printoptions(threshold=float('inf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 145])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input = torch.zeros(128, 1, 145) \n",
    "for i, _ in enumerate(enc):\n",
    "    input[i, 0, _.data] = 100\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8399)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(input.log_softmax(2), \n",
    "          enc, \n",
    "          torch.asarray([128]),\n",
    "          len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ãōd0ãðíèðèv1đù(ð òàùvũgèggnöösugzröìð:ö:gðơðmàeeô/-è-èqè:jjvhh((àààwm|ha_+íùhz\\\\ìđăiđ'}g-gcoùùùs\\\\%đ0wwáàà:àööù%ðleðg/ðpa8ðððeöeèj\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, preds_id  = preds_.max(2)\n",
    "converter.decode(preds_id.view(-1), preds_lengths, raw=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def cer_loss_one_image(sim_pred, label):\n",
    "    loss = Levenshtein.distance(sim_pred, label) * 1.0 \n",
    "    return loss\n",
    "\n",
    "def cer_loss(sim_preds, labels):\n",
    "    losses = []\n",
    "    print(sim_preds.__len__())\n",
    "    for i in range(sim_preds.__len__()):\n",
    "        pred = sim_preds[i]\n",
    "        text = labels[i]\n",
    "\n",
    "        loss = cer_loss_one_image(pred, text)\n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cer_loss(['hello0',], ['helllo',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tandat17z",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
