{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\HUST\\20232\\ML\\Project_OCR\\HandwritingRecognition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> use MyCrnn-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model.MyCrnn import MyCRNN\n",
    "\n",
    "model = MyCRNN(155, 200, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0275, -0.0924,  0.0000,  ..., -0.0799, -0.0733,  0.0572],\n",
       "         [-0.1076, -0.1926, -0.0461,  ..., -0.0652,  0.0899, -0.0536],\n",
       "         [-0.2209, -0.1007, -0.0216,  ..., -0.1314,  0.0491, -0.0223],\n",
       "         ...,\n",
       "         [-0.0256, -0.1311, -0.0529,  ...,  0.0027, -0.0747, -0.1859],\n",
       "         [ 0.0232, -0.1240, -0.0956,  ...,  0.0703, -0.0354, -0.2528],\n",
       "         [ 0.0185, -0.0377,  0.1235,  ...,  0.0835, -0.1130, -0.0398]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(1, 1, 32, 512)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tandat17z\\anaconda3\\envs\\env_tandat17z\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.crnn import CRNN\n",
    "from model.MyCrnn import MyCRNN\n",
    "from dataset import *\n",
    "from utils.StrLabelConverter import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Using cpu device\n",
      "---------------------------------------------------\n",
      "Num class:  155\n"
     ]
    }
   ],
   "source": [
    "device = ( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"Using {device} device\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "# --------------Tạo Dataset -------------------------------------------------------\n",
    "train_dataset = DatasetImg('data/train' + '/img', 'data/train' + '/label')\n",
    "test_dataset = DatasetImg('data/test' + '/img', 'data/test' + '/label')\n",
    "\n",
    "with open(os.path.join('data/mychar.txt'), 'r', encoding='utf-8') as f:\n",
    "    alphabet = f.read().rstrip()\n",
    "converter = StrLabelConverter(alphabet)\n",
    "print('Num class: ', converter.numClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> use MyCrnn-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # --------------------- Create Model ---------------------------------\n",
    "model = MyCRNN(converter.numClass, 200, 0.1).to(device)\n",
    "# print(f\"Model structure: {model}\\n\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "criterion = torch.nn.CTCLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tester import Tester\n",
    "from trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model, \n",
    "                optimizer = optimizer,\n",
    "                criterion = criterion, \n",
    "                converter = converter,\n",
    "                train_dataset = train_dataset,\n",
    "                batch_size = 64)\n",
    "\n",
    "tester = Tester(model,\n",
    "                criterion = criterion, \n",
    "                converter = converter,\n",
    "                test_dataset = test_dataset,\n",
    "                batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/78 [00:50<1:05:20, 50.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m total_loss, levenshtein_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m avg_Loss = \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Levenshtein Loss per 1 sentence = \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m1\u001b[39m, start_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, total_loss, levenshtein_loss))\n",
      "File \u001b[1;32md:\\HUST\\20232\\ML\\Project_OCR\\HandwritingRecognition\\trainer.py:50\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Backward -------------------------------------------------\u001b[39;00m\n\u001b[0;32m     49\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     53\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\tandat17z\\anaconda3\\envs\\env_tandat17z\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tandat17z\\anaconda3\\envs\\env_tandat17z\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(True)\n",
    "total_loss, levenshtein_loss = trainer.train()\n",
    "print('Epoch: [{}/{}]\\t avg_Loss = {:.4f} \\t Levenshtein Loss per 1 sentence = {:.2f}'.format(1, start_epoch + 1, total_loss, levenshtein_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tester.eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "raw: 666RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRàà\n",
      "pred_text: 6Rà\n",
      "gt: hoàn cảnh để theo đuổi ước mơ từ ngày thơ bé.\n",
      "==============================\n",
      "raw: 666RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRàà\n",
      "pred_text: 6Rà\n",
      "gt: hợp thành phòng tuyến bảo vệ và làm chủ vùng biển.\n",
      "==============================\n",
      "raw: 666RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRyyyyRRRRRRRRRRRRRRRRRRRRRyyyyyyRRRRRRRRRRRRRRRRRRRRyyRRRRRRRRRRRRRRRRRRRRRRRRRRRyyRRRRyyyyyyRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRyyyyyRRRRRRRRRRRRRyyRRRRRRRRRRàà\n",
      "pred_text: 6RyRyRyRyRyRyRyRà\n",
      "gt: khác của từ thành đạt, nghĩa là có được một cuộc sống giàu sang, được mọi người nể phục?\n",
      "==============================\n",
      "raw: 666RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRàà\n",
      "pred_text: 6Rà\n",
      "gt: phẩy năm? Đó thật ra không phải là thất bại, chỉ là khi thành công - bị - trì - hoãn mà\n",
      "==============================\n",
      "raw: 666RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRàà\n",
      "pred_text: 6Rà\n",
      "gt: Đã bao giờ bạn tự hỏi thành công là gì mà bao kẻ bỏ cả cuộc đời mình theo đuổi? Phải\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(163.47359619140624, 69.8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss, levenshtein_loss = tester.eval()\n",
    "print('--> Val: \\t avg_Loss = {:.4f} \\t Levenshtein Loss per 1 sentence = {:.2f}'.format(total_loss, levenshtein_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, nclass, num_hidden, dropout = 0):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        ks = [ 3,   3,   3,   3,   3,   3,   1,   1,   1]\n",
    "        ss = [ 1,   1,   1,   1,   1,   1,   1,   1,   1]\n",
    "        ps = [ 1,   1,   1,   1,   1,   1,   1,   1,   1]\n",
    "        nm = [64, 128, 256, 256, 256, 512, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "        def convRelu(i):\n",
    "            nIn = 1 if i == 0 else nm[i - 1] \n",
    "            nOut = nm[i]\n",
    "            # cnn.add_module('conv{0}'.format(i),\n",
    "            #                nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            cnn.add_module('conv{0}'.format(i), nn.Conv2d(nIn, nOut, 3, 1, 1))\n",
    "            cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        # input : (C, H, W) - (1, 32, 512)\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d((2, 2)))  # 64, 16, 256\n",
    "        convRelu(1) \n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d((2, 2)))  # 128, 8, 128\n",
    "        convRelu(2) \n",
    "        convRelu(3) \n",
    "        cnn.add_module('pooling{0}'.format(2), nn.MaxPool2d((2, 1)))  # 256, 4, 128\n",
    "        convRelu(4) \n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d((2, 1)))  # 512, 2, 128\n",
    "        convRelu(6) \n",
    "        convRelu(7)\n",
    "        cnn.add_module('pooling{0}'.format(4), nn.MaxPool2d((2, 1)))  # 512, 1, 128\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        # BiLSTM\n",
    "        self.biLSTM1 = nn.LSTM(512, num_hidden, bidirectional=True, batch_first = True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(num_hidden * 2, nclass, bias = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        x1 = self.cnn(input) # b, 512, 1, 128\n",
    "        x1 = torch.squeeze(x1, 2) # b, 512, 128\n",
    "        x1 = x1.permute(0, 2, 1)  # b, 128, 512\n",
    "\n",
    "        x2, _  = self.biLSTM1(x1) # b, 128, num_hidden*2\n",
    "        x2 = self.dropout1(x2) \n",
    "\n",
    "        x3 = self.linear(x2) # b, 128, num_class\n",
    "        out = self.dropout(x3)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 124])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_ = CRNN(124, 100)\n",
    "out = cnn_(torch.rand(64, 1, 32, 512))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "    (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu7): ReLU(inplace=True)\n",
       "    (pooling4): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (biLSTM1): LSTM(512, 100, batch_first=True, bidirectional=True)\n",
       "  (dropout1): Dropout(p=0, inplace=False)\n",
       "  (linear): Linear(in_features=200, out_features=124, bias=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: cnn.conv0.weight | Size: torch.Size([64, 1, 3, 3]) | Values : tensor([[[[ 0.1159, -0.0922, -0.0774],\n",
      "          [-0.0084, -0.2158,  0.0290],\n",
      "          [-0.3093,  0.2286, -0.3006]]],\n",
      "\n",
      "\n",
      "        [[[-0.0702,  0.1410, -0.0326],\n",
      "          [ 0.2566,  0.0519,  0.0762],\n",
      "          [-0.1566,  0.0275, -0.0293]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv0.bias | Size: torch.Size([64]) | Values : tensor([-0.2395,  0.2840], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm0.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm0.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv1.weight | Size: torch.Size([128, 64, 3, 3]) | Values : tensor([[[[ 0.0385, -0.0375, -0.0349],\n",
      "          [ 0.0327, -0.0155, -0.0409],\n",
      "          [-0.0165, -0.0222, -0.0108]],\n",
      "\n",
      "         [[ 0.0355,  0.0303,  0.0415],\n",
      "          [ 0.0296,  0.0169, -0.0204],\n",
      "          [ 0.0227, -0.0389,  0.0119]],\n",
      "\n",
      "         [[ 0.0374,  0.0096,  0.0357],\n",
      "          [ 0.0135,  0.0097,  0.0378],\n",
      "          [ 0.0343, -0.0051, -0.0267]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0257,  0.0380,  0.0095],\n",
      "          [-0.0335, -0.0186,  0.0248],\n",
      "          [-0.0400, -0.0254, -0.0192]],\n",
      "\n",
      "         [[ 0.0405,  0.0013, -0.0178],\n",
      "          [-0.0173, -0.0323,  0.0231],\n",
      "          [ 0.0197, -0.0399,  0.0352]],\n",
      "\n",
      "         [[ 0.0007, -0.0020,  0.0003],\n",
      "          [ 0.0327, -0.0329, -0.0411],\n",
      "          [ 0.0362, -0.0216,  0.0298]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036,  0.0176, -0.0133],\n",
      "          [ 0.0205, -0.0349, -0.0380],\n",
      "          [-0.0075,  0.0291, -0.0072]],\n",
      "\n",
      "         [[-0.0186, -0.0056,  0.0024],\n",
      "          [ 0.0256,  0.0100,  0.0058],\n",
      "          [ 0.0290,  0.0102, -0.0272]],\n",
      "\n",
      "         [[ 0.0072,  0.0361,  0.0387],\n",
      "          [ 0.0058, -0.0093,  0.0123],\n",
      "          [-0.0348,  0.0025,  0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0157, -0.0174, -0.0367],\n",
      "          [ 0.0080, -0.0195, -0.0207],\n",
      "          [ 0.0174, -0.0170, -0.0235]],\n",
      "\n",
      "         [[ 0.0121, -0.0127, -0.0076],\n",
      "          [ 0.0053,  0.0260,  0.0109],\n",
      "          [ 0.0176,  0.0096,  0.0359]],\n",
      "\n",
      "         [[-0.0012,  0.0367, -0.0341],\n",
      "          [-0.0163,  0.0162, -0.0009],\n",
      "          [ 0.0313, -0.0364, -0.0193]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv1.bias | Size: torch.Size([128]) | Values : tensor([-0.0359,  0.0341], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm1.weight | Size: torch.Size([128]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm1.bias | Size: torch.Size([128]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv2.weight | Size: torch.Size([128, 128, 3, 3]) | Values : tensor([[[[ 0.0098,  0.0174,  0.0048],\n",
      "          [-0.0120, -0.0247, -0.0219],\n",
      "          [-0.0033,  0.0280, -0.0100]],\n",
      "\n",
      "         [[ 0.0082,  0.0228,  0.0015],\n",
      "          [-0.0134, -0.0190, -0.0075],\n",
      "          [ 0.0007, -0.0150, -0.0045]],\n",
      "\n",
      "         [[ 0.0084, -0.0049, -0.0206],\n",
      "          [ 0.0072, -0.0214,  0.0231],\n",
      "          [-0.0252, -0.0180, -0.0162]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0130, -0.0090,  0.0201],\n",
      "          [ 0.0074, -0.0126, -0.0105],\n",
      "          [-0.0255,  0.0156, -0.0175]],\n",
      "\n",
      "         [[-0.0234,  0.0075, -0.0225],\n",
      "          [ 0.0205, -0.0156,  0.0198],\n",
      "          [ 0.0164,  0.0091,  0.0108]],\n",
      "\n",
      "         [[ 0.0097, -0.0265, -0.0118],\n",
      "          [ 0.0141,  0.0176, -0.0236],\n",
      "          [-0.0078,  0.0271,  0.0150]]],\n",
      "\n",
      "\n",
      "        [[[-0.0235, -0.0273,  0.0228],\n",
      "          [ 0.0233,  0.0124, -0.0055],\n",
      "          [ 0.0156, -0.0110, -0.0285]],\n",
      "\n",
      "         [[-0.0108,  0.0096, -0.0257],\n",
      "          [-0.0013,  0.0268,  0.0238],\n",
      "          [-0.0073, -0.0053, -0.0178]],\n",
      "\n",
      "         [[ 0.0011,  0.0244, -0.0116],\n",
      "          [-0.0090,  0.0141, -0.0154],\n",
      "          [-0.0192, -0.0090, -0.0237]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0257,  0.0001,  0.0014],\n",
      "          [ 0.0229,  0.0225, -0.0097],\n",
      "          [ 0.0146, -0.0269,  0.0203]],\n",
      "\n",
      "         [[-0.0192,  0.0053, -0.0220],\n",
      "          [ 0.0268, -0.0180,  0.0255],\n",
      "          [-0.0033, -0.0226, -0.0103]],\n",
      "\n",
      "         [[-0.0048, -0.0287,  0.0075],\n",
      "          [ 0.0095,  0.0224, -0.0064],\n",
      "          [-0.0168, -0.0145, -0.0223]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv2.bias | Size: torch.Size([128]) | Values : tensor([ 0.0017, -0.0026], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm2.weight | Size: torch.Size([128]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm2.bias | Size: torch.Size([128]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv3.weight | Size: torch.Size([256, 128, 3, 3]) | Values : tensor([[[[ 0.0140, -0.0269, -0.0153],\n",
      "          [-0.0253,  0.0171, -0.0082],\n",
      "          [ 0.0069, -0.0256, -0.0132]],\n",
      "\n",
      "         [[ 0.0273, -0.0102,  0.0033],\n",
      "          [ 0.0058, -0.0190, -0.0011],\n",
      "          [ 0.0051,  0.0185,  0.0091]],\n",
      "\n",
      "         [[ 0.0191,  0.0046,  0.0185],\n",
      "          [ 0.0142,  0.0032,  0.0225],\n",
      "          [-0.0167,  0.0232, -0.0195]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0256, -0.0243,  0.0115],\n",
      "          [-0.0009,  0.0233, -0.0155],\n",
      "          [ 0.0080,  0.0114,  0.0024]],\n",
      "\n",
      "         [[-0.0026,  0.0073,  0.0071],\n",
      "          [-0.0173, -0.0089, -0.0252],\n",
      "          [-0.0144,  0.0164,  0.0093]],\n",
      "\n",
      "         [[-0.0264, -0.0162, -0.0069],\n",
      "          [ 0.0069, -0.0121, -0.0179],\n",
      "          [ 0.0192,  0.0196, -0.0139]]],\n",
      "\n",
      "\n",
      "        [[[-0.0185,  0.0185,  0.0216],\n",
      "          [-0.0155,  0.0124, -0.0288],\n",
      "          [-0.0179,  0.0205, -0.0221]],\n",
      "\n",
      "         [[-0.0084, -0.0045,  0.0261],\n",
      "          [ 0.0170,  0.0220,  0.0036],\n",
      "          [ 0.0022, -0.0288,  0.0125]],\n",
      "\n",
      "         [[ 0.0099, -0.0164, -0.0292],\n",
      "          [-0.0003,  0.0025,  0.0280],\n",
      "          [ 0.0215,  0.0006, -0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0037,  0.0150, -0.0075],\n",
      "          [ 0.0276,  0.0182, -0.0225],\n",
      "          [ 0.0062, -0.0259, -0.0258]],\n",
      "\n",
      "         [[-0.0238,  0.0285, -0.0181],\n",
      "          [-0.0062, -0.0128, -0.0252],\n",
      "          [ 0.0055,  0.0217,  0.0282]],\n",
      "\n",
      "         [[-0.0035,  0.0067,  0.0043],\n",
      "          [-0.0238, -0.0190,  0.0015],\n",
      "          [-0.0012, -0.0242, -0.0162]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv3.bias | Size: torch.Size([256]) | Values : tensor([-0.0161, -0.0055], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm3.weight | Size: torch.Size([256]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm3.bias | Size: torch.Size([256]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv4.weight | Size: torch.Size([256, 256, 3, 3]) | Values : tensor([[[[ 0.0012, -0.0133,  0.0112],\n",
      "          [-0.0016,  0.0083, -0.0054],\n",
      "          [ 0.0045, -0.0115,  0.0093]],\n",
      "\n",
      "         [[ 0.0032, -0.0165, -0.0177],\n",
      "          [ 0.0038, -0.0063, -0.0102],\n",
      "          [-0.0149, -0.0179, -0.0068]],\n",
      "\n",
      "         [[ 0.0043,  0.0086, -0.0098],\n",
      "          [ 0.0169,  0.0167,  0.0130],\n",
      "          [-0.0127, -0.0174, -0.0130]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0184, -0.0034, -0.0034],\n",
      "          [-0.0143,  0.0045, -0.0070],\n",
      "          [ 0.0146,  0.0090, -0.0088]],\n",
      "\n",
      "         [[ 0.0082, -0.0172, -0.0188],\n",
      "          [-0.0029, -0.0120,  0.0207],\n",
      "          [ 0.0071,  0.0120,  0.0092]],\n",
      "\n",
      "         [[ 0.0110, -0.0095, -0.0079],\n",
      "          [-0.0058,  0.0062,  0.0093],\n",
      "          [ 0.0178,  0.0163, -0.0009]]],\n",
      "\n",
      "\n",
      "        [[[-0.0057, -0.0050,  0.0161],\n",
      "          [-0.0173, -0.0003, -0.0154],\n",
      "          [ 0.0097,  0.0189,  0.0151]],\n",
      "\n",
      "         [[-0.0074,  0.0169, -0.0039],\n",
      "          [-0.0111, -0.0194, -0.0167],\n",
      "          [-0.0129,  0.0127,  0.0084]],\n",
      "\n",
      "         [[-0.0041,  0.0122,  0.0185],\n",
      "          [ 0.0139, -0.0091,  0.0056],\n",
      "          [-0.0183, -0.0022,  0.0143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0023, -0.0145, -0.0039],\n",
      "          [-0.0088,  0.0089, -0.0062],\n",
      "          [ 0.0037,  0.0120,  0.0081]],\n",
      "\n",
      "         [[-0.0032,  0.0156, -0.0035],\n",
      "          [ 0.0162,  0.0073,  0.0097],\n",
      "          [ 0.0197,  0.0086,  0.0035]],\n",
      "\n",
      "         [[ 0.0138,  0.0152, -0.0062],\n",
      "          [ 0.0102,  0.0185,  0.0100],\n",
      "          [-0.0188, -0.0055, -0.0170]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv4.bias | Size: torch.Size([256]) | Values : tensor([-0.0045,  0.0196], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm4.weight | Size: torch.Size([256]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm4.bias | Size: torch.Size([256]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv5.weight | Size: torch.Size([512, 256, 3, 3]) | Values : tensor([[[[-5.2392e-04,  1.4282e-02,  1.7927e-02],\n",
      "          [-1.8987e-02, -1.1316e-02, -1.3873e-02],\n",
      "          [ 1.1543e-02, -1.4394e-02, -2.0237e-02]],\n",
      "\n",
      "         [[-1.3556e-02,  1.2339e-02, -9.5207e-03],\n",
      "          [ 3.1045e-03,  5.4448e-03, -1.1988e-02],\n",
      "          [ 1.9063e-02, -1.9103e-03,  9.9915e-03]],\n",
      "\n",
      "         [[-5.2700e-03, -1.6454e-02,  4.0227e-03],\n",
      "          [-3.5082e-03,  1.1289e-02, -7.9002e-04],\n",
      "          [ 8.4851e-04,  1.8034e-02,  1.4488e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0056e-02,  2.9534e-03, -4.4502e-03],\n",
      "          [-1.0878e-02,  1.9090e-02,  1.5800e-03],\n",
      "          [-1.6499e-03, -1.4303e-02,  2.0577e-02]],\n",
      "\n",
      "         [[-1.9564e-02,  4.5523e-03, -9.2791e-03],\n",
      "          [-3.8313e-03, -6.3101e-03,  1.9779e-02],\n",
      "          [ 7.5752e-03, -1.2657e-02, -2.0309e-04]],\n",
      "\n",
      "         [[ 3.9003e-04,  3.5831e-03, -1.2742e-02],\n",
      "          [ 1.8475e-02, -3.5799e-03,  1.0175e-03],\n",
      "          [-9.8513e-05,  1.8291e-02, -8.2514e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.7962e-02,  4.9331e-03,  1.9971e-02],\n",
      "          [-2.0400e-02, -1.1543e-02,  1.7658e-02],\n",
      "          [ 4.6525e-03,  4.6134e-03, -1.4979e-02]],\n",
      "\n",
      "         [[ 4.7038e-03, -1.5978e-02,  1.9651e-02],\n",
      "          [ 1.8260e-02, -1.6414e-02,  1.7017e-02],\n",
      "          [ 3.1004e-04,  5.4100e-03, -2.0613e-02]],\n",
      "\n",
      "         [[-9.4370e-03,  1.3011e-02,  1.4146e-02],\n",
      "          [-2.0785e-03,  1.8565e-02,  1.8563e-02],\n",
      "          [-2.9047e-03,  1.1311e-02, -1.3513e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5290e-05, -8.1031e-03, -1.1413e-02],\n",
      "          [-1.1337e-02,  1.3682e-02,  1.5184e-02],\n",
      "          [-1.3299e-03, -3.4721e-03,  1.4401e-02]],\n",
      "\n",
      "         [[ 1.2008e-02, -1.4137e-02, -1.4221e-02],\n",
      "          [-1.0214e-02,  1.4263e-02,  1.8432e-02],\n",
      "          [-1.4819e-02, -1.1264e-02,  7.6885e-03]],\n",
      "\n",
      "         [[-1.7166e-02, -7.7484e-03, -1.6840e-02],\n",
      "          [-9.8598e-03,  1.1854e-02, -1.3389e-02],\n",
      "          [ 9.3272e-03,  1.2323e-02, -1.2469e-02]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv5.bias | Size: torch.Size([512]) | Values : tensor([9.7811e-05, 1.7900e-02], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm5.weight | Size: torch.Size([512]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm5.bias | Size: torch.Size([512]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear1.weight | Size: torch.Size([128, 64]) | Values : tensor([[-0.0406, -0.1229,  0.0023, -0.0873, -0.0873,  0.0666, -0.0134, -0.0587,\n",
      "          0.0211,  0.1031,  0.0113,  0.0963,  0.0513,  0.0843,  0.1247, -0.0459,\n",
      "          0.0632, -0.0901,  0.0033,  0.0880, -0.0596, -0.0082,  0.0088,  0.0142,\n",
      "         -0.0297,  0.1218,  0.0081, -0.1159, -0.0482,  0.0317, -0.1100, -0.0239,\n",
      "         -0.0597, -0.1041, -0.0801, -0.0891, -0.0551,  0.0079, -0.0093, -0.1186,\n",
      "         -0.0669, -0.0457, -0.0501, -0.1050, -0.0599, -0.0774,  0.0112,  0.0747,\n",
      "         -0.0619,  0.0588,  0.0915,  0.0913,  0.0111,  0.1239,  0.0878, -0.0267,\n",
      "         -0.0859, -0.0971,  0.1234, -0.0130, -0.0790, -0.0901, -0.0646,  0.0156],\n",
      "        [ 0.1112,  0.1172, -0.1067, -0.0032, -0.1031, -0.0554, -0.0841, -0.0787,\n",
      "         -0.0785, -0.0480,  0.0706,  0.1030,  0.0282, -0.0007, -0.1072, -0.0206,\n",
      "          0.0246,  0.0028, -0.0695, -0.1213,  0.0815, -0.0409,  0.1191, -0.0767,\n",
      "          0.0934,  0.0300,  0.1005, -0.0045, -0.0227, -0.1041, -0.0979, -0.1211,\n",
      "          0.0110,  0.0426,  0.0567, -0.1204, -0.1063,  0.0312, -0.0919, -0.0667,\n",
      "         -0.0158, -0.1037,  0.0821, -0.0629,  0.0009,  0.0855, -0.0243,  0.0224,\n",
      "         -0.0797,  0.0707,  0.0483,  0.0629,  0.0773,  0.0870, -0.0720, -0.0457,\n",
      "          0.0342,  0.0285,  0.0684,  0.0267, -0.0922,  0.0922,  0.0998, -0.0915]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear1.bias | Size: torch.Size([128]) | Values : tensor([0.0808, 0.0235], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_ih_l0 | Size: torch.Size([400, 512]) | Values : tensor([[-0.0843,  0.0494,  0.0253,  ..., -0.0349, -0.0668, -0.0133],\n",
      "        [-0.0922, -0.0750,  0.0565,  ...,  0.0884, -0.0987,  0.0407]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_hh_l0 | Size: torch.Size([400, 100]) | Values : tensor([[ 0.0123, -0.0297, -0.0457,  0.0953, -0.0260, -0.0597,  0.0792,  0.0800,\n",
      "          0.0013, -0.0095, -0.0840, -0.0578,  0.0473, -0.0190, -0.0417,  0.0234,\n",
      "         -0.0789, -0.0851,  0.0315,  0.0026, -0.0376,  0.0263,  0.0006, -0.0307,\n",
      "          0.0301, -0.0928, -0.0752, -0.0370, -0.0042, -0.0795, -0.0900, -0.0897,\n",
      "         -0.0760,  0.0588,  0.0398,  0.0767,  0.0128, -0.0806, -0.0769, -0.0790,\n",
      "          0.0967,  0.0896,  0.0647,  0.0352, -0.0574, -0.0585,  0.0272, -0.0449,\n",
      "          0.0327,  0.0477,  0.0889, -0.0355,  0.0786,  0.0209, -0.0046,  0.0332,\n",
      "          0.0848,  0.0757, -0.0927, -0.0561,  0.0153,  0.0222, -0.0058,  0.0987,\n",
      "         -0.0150, -0.0269, -0.0163,  0.0933, -0.0463,  0.0692, -0.0629, -0.0336,\n",
      "         -0.0209,  0.0337, -0.0698,  0.0684,  0.0038, -0.0716, -0.0202, -0.0516,\n",
      "         -0.0125,  0.0388,  0.0806, -0.0615,  0.0596,  0.0713, -0.0387, -0.0960,\n",
      "         -0.0935,  0.0669,  0.0652,  0.0477, -0.0086, -0.0156,  0.0023, -0.0830,\n",
      "          0.0985,  0.0767, -0.0696, -0.0886],\n",
      "        [ 0.0467,  0.0938, -0.0174, -0.0524,  0.0284,  0.0709, -0.0159,  0.0080,\n",
      "         -0.0863, -0.0433, -0.0211,  0.0247, -0.0410, -0.0437, -0.0946,  0.0361,\n",
      "          0.0526,  0.0219,  0.0066, -0.0775,  0.0335,  0.0924,  0.0320,  0.0176,\n",
      "          0.0158,  0.0886, -0.0033, -0.0793, -0.0761, -0.0067, -0.0948, -0.0287,\n",
      "         -0.0009,  0.0517, -0.0494, -0.0528, -0.0579,  0.0493,  0.0202, -0.0954,\n",
      "         -0.0849,  0.0208,  0.0111, -0.0791, -0.0712, -0.0323,  0.0276,  0.0975,\n",
      "          0.0639,  0.0060,  0.0403,  0.0711,  0.0605,  0.0888, -0.0715, -0.0398,\n",
      "         -0.0282, -0.0260, -0.0739,  0.0097, -0.0514,  0.0257,  0.0697, -0.0279,\n",
      "         -0.0441,  0.0633, -0.0949, -0.0374, -0.0422, -0.0362,  0.0236,  0.0900,\n",
      "          0.0597,  0.0167,  0.0626, -0.0990,  0.0086,  0.0209,  0.0121, -0.0217,\n",
      "         -0.0670, -0.0707, -0.0037, -0.0619,  0.0069, -0.0142,  0.0016,  0.0490,\n",
      "         -0.0257,  0.0434, -0.0973, -0.0083,  0.0574, -0.0563, -0.0197, -0.0685,\n",
      "         -0.0382, -0.0127, -0.0297,  0.0603]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_ih_l0 | Size: torch.Size([400]) | Values : tensor([-0.0405,  0.0041], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_hh_l0 | Size: torch.Size([400]) | Values : tensor([ 0.0247, -0.0325], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_ih_l0_reverse | Size: torch.Size([400, 512]) | Values : tensor([[ 0.0486,  0.0459, -0.0044,  ...,  0.0371, -0.0451,  0.0581],\n",
      "        [-0.0834,  0.0583,  0.0047,  ...,  0.0515,  0.0734,  0.0519]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_hh_l0_reverse | Size: torch.Size([400, 100]) | Values : tensor([[-0.0380, -0.0633, -0.0883, -0.0190,  0.0357,  0.0547, -0.0580,  0.0596,\n",
      "         -0.0965,  0.0388, -0.0159,  0.0059, -0.0865, -0.0667, -0.0937, -0.0431,\n",
      "         -0.0364,  0.0664,  0.0887, -0.0091, -0.0261, -0.0469, -0.0393,  0.0526,\n",
      "         -0.0260,  0.0878, -0.0488, -0.0759,  0.0585, -0.0930, -0.0001, -0.0496,\n",
      "          0.0309, -0.0839, -0.0391, -0.0018, -0.0739, -0.0201,  0.0536, -0.0713,\n",
      "         -0.0354, -0.0250,  0.0581,  0.0278, -0.0883,  0.0150,  0.0439,  0.0994,\n",
      "          0.0679, -0.0992,  0.0071, -0.0332, -0.0964,  0.0752, -0.0557, -0.0830,\n",
      "          0.0141,  0.0531, -0.0343, -0.0437, -0.0870,  0.0592,  0.0074, -0.0036,\n",
      "          0.0959, -0.0600, -0.0432,  0.0497,  0.0386, -0.0884, -0.0304,  0.0862,\n",
      "         -0.0427, -0.0249,  0.0456,  0.0546, -0.0623,  0.0687, -0.0824,  0.0213,\n",
      "          0.0100,  0.0251,  0.0207,  0.0800,  0.0162,  0.0742,  0.0115, -0.0512,\n",
      "         -0.0419,  0.0335, -0.0934,  0.0077, -0.0987, -0.0089,  0.0531, -0.0066,\n",
      "          0.0871,  0.0036,  0.0157, -0.0597],\n",
      "        [-0.0774, -0.0009, -0.0813,  0.0535, -0.0235,  0.0454, -0.0586, -0.0745,\n",
      "         -0.0994,  0.0773,  0.0927, -0.0903, -0.0586, -0.0518, -0.0439, -0.0821,\n",
      "          0.0323,  0.0651,  0.0179,  0.0338,  0.0879, -0.0100,  0.0343,  0.0003,\n",
      "         -0.0564,  0.0091, -0.0344, -0.0848, -0.0225,  0.0393, -0.0685,  0.0584,\n",
      "         -0.0931,  0.0409,  0.0205,  0.0583, -0.0348,  0.0292,  0.0688, -0.0764,\n",
      "          0.0294, -0.0952,  0.0878, -0.0361, -0.0177, -0.0205,  0.0416,  0.0548,\n",
      "          0.0615, -0.0150, -0.0102,  0.0535, -0.0178,  0.0736,  0.0963,  0.0816,\n",
      "         -0.0844, -0.0129, -0.0725,  0.0700, -0.0011, -0.0469, -0.0150,  0.0078,\n",
      "          0.0877,  0.0467,  0.0501, -0.0158, -0.0008,  0.0625, -0.0290, -0.0516,\n",
      "         -0.0495,  0.0494,  0.0087, -0.0915,  0.0413,  0.0418, -0.0226,  0.0045,\n",
      "         -0.0069, -0.0623,  0.0139, -0.0182,  0.0225, -0.0036, -0.0089,  0.0207,\n",
      "          0.0610, -0.0479, -0.0822,  0.0766,  0.0302,  0.0669, -0.0108,  0.0261,\n",
      "         -0.0414,  0.0341,  0.0617, -0.0697]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_ih_l0_reverse | Size: torch.Size([400]) | Values : tensor([0.0368, 0.0252], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_hh_l0_reverse | Size: torch.Size([400]) | Values : tensor([-0.0012, -0.0480], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear2.weight | Size: torch.Size([124, 200]) | Values : tensor([[ 2.9494e-02,  6.5491e-02, -5.7590e-02, -4.7205e-02, -5.3332e-02,\n",
      "         -1.8921e-02, -6.3015e-02,  5.8128e-02, -3.0939e-02,  3.1307e-02,\n",
      "          2.7958e-02,  1.4704e-02,  9.5869e-03, -7.1495e-03, -3.9242e-02,\n",
      "          1.0336e-02,  1.7041e-02, -3.3502e-02, -5.9951e-02, -5.4861e-02,\n",
      "         -5.6885e-02, -1.3631e-03, -3.4018e-02, -6.3891e-02, -6.8314e-02,\n",
      "         -4.5317e-02, -3.0445e-02,  2.3836e-03,  1.9910e-02, -1.9392e-02,\n",
      "         -2.6338e-02, -1.8305e-02, -4.0556e-02,  2.5726e-02,  5.7145e-02,\n",
      "          2.9603e-02, -2.2051e-02,  4.3138e-02,  6.6595e-02, -3.4166e-02,\n",
      "         -2.9343e-02, -7.0173e-02,  4.4687e-02,  3.7977e-02,  2.4513e-02,\n",
      "         -5.5577e-02, -6.9741e-02,  1.9391e-02,  2.3481e-02, -5.1319e-02,\n",
      "          3.6582e-02, -5.5459e-02, -2.8874e-02,  1.3981e-02, -5.1651e-03,\n",
      "          1.8653e-02,  8.2540e-03,  5.2821e-02, -2.2520e-02,  6.6553e-02,\n",
      "          3.9943e-02,  1.6818e-02, -5.1220e-03, -1.3070e-02, -2.8808e-02,\n",
      "          3.6909e-02,  6.5986e-02,  1.7970e-02, -1.4629e-02,  3.2585e-02,\n",
      "          6.0972e-02, -3.4600e-05,  6.5427e-02,  3.3894e-02, -5.6739e-02,\n",
      "         -3.2038e-02,  1.3067e-02, -1.3017e-02,  5.1911e-02,  5.0002e-02,\n",
      "          5.9934e-02,  2.5886e-02,  5.8120e-03, -1.3786e-02,  3.1384e-02,\n",
      "          5.5555e-03,  1.3490e-02, -2.3639e-02, -4.4613e-02, -5.7455e-02,\n",
      "         -3.8057e-02,  2.1600e-02, -4.5314e-02,  5.7062e-02, -3.5031e-02,\n",
      "         -3.2884e-02,  2.0577e-02, -2.7701e-02, -6.3263e-05,  5.3310e-02,\n",
      "         -4.2577e-02, -3.5264e-02,  6.0446e-03,  5.2341e-02,  2.7950e-02,\n",
      "          3.6590e-02,  2.8273e-02, -5.3243e-02, -1.9850e-02, -2.2753e-02,\n",
      "         -6.6494e-02,  1.8394e-04,  1.5386e-02, -4.6621e-02, -1.7630e-02,\n",
      "         -5.3794e-02, -1.3127e-02, -1.5287e-02, -6.2072e-02, -1.5861e-02,\n",
      "          3.7509e-02, -5.0312e-02, -1.4645e-02,  3.5279e-02,  5.4137e-02,\n",
      "         -1.6853e-02,  4.6877e-02, -6.1983e-02, -1.8474e-02,  5.8453e-02,\n",
      "         -6.5283e-03, -3.1387e-02,  5.8944e-02, -4.0292e-02, -4.3028e-02,\n",
      "          3.1448e-02,  3.3784e-02, -2.5220e-02,  6.4743e-02, -1.7155e-02,\n",
      "          5.7514e-02,  3.1684e-02,  2.4706e-02, -6.8471e-03, -5.6142e-02,\n",
      "          3.8959e-02,  2.8065e-02,  4.1284e-02,  3.9892e-02,  4.6422e-02,\n",
      "          4.6289e-02,  1.9727e-02,  3.0843e-02, -6.2048e-02,  4.7252e-02,\n",
      "         -3.3977e-02,  6.8966e-02,  5.5082e-02, -1.4348e-02,  1.0123e-02,\n",
      "         -6.9586e-03, -1.8928e-02, -3.8595e-02,  7.0637e-02,  1.6594e-02,\n",
      "          4.0099e-02,  1.1823e-02,  1.0002e-02, -2.6243e-03, -6.5739e-02,\n",
      "         -6.7144e-02, -1.0417e-02,  6.8071e-02, -1.3721e-02, -5.5599e-02,\n",
      "          6.9052e-02,  1.5551e-02, -2.3476e-02, -4.2887e-02,  1.3917e-02,\n",
      "          6.3411e-02, -7.0321e-02, -2.4008e-02, -2.1076e-02, -6.9703e-02,\n",
      "          3.9334e-02,  1.9025e-02, -4.0427e-03,  2.7559e-03, -3.5028e-02,\n",
      "         -6.5715e-02,  2.5836e-02, -4.6632e-03,  2.7409e-02,  5.5346e-02,\n",
      "         -6.7990e-02,  5.9181e-02,  5.1851e-02,  4.7330e-02,  4.0389e-02],\n",
      "        [-6.7480e-03, -8.6381e-03, -4.6758e-03, -4.5668e-02, -1.9818e-02,\n",
      "          6.4190e-02,  6.9930e-02,  6.3735e-02, -6.5352e-02,  3.2960e-02,\n",
      "         -6.1792e-02,  6.5363e-02,  5.3035e-02, -6.9594e-02,  1.4004e-02,\n",
      "         -4.1613e-02, -6.0407e-03, -5.0391e-02,  4.1401e-02, -1.7549e-02,\n",
      "         -6.0390e-02, -5.6454e-03,  2.6692e-02, -3.0175e-02,  3.1639e-02,\n",
      "          3.1301e-02,  3.3278e-02,  7.1613e-04,  3.8975e-02,  6.2811e-02,\n",
      "          7.3780e-03,  5.9974e-02, -1.0318e-02, -5.4778e-02, -5.4925e-02,\n",
      "         -3.9283e-02, -2.7101e-02,  1.4597e-02, -8.3224e-03, -8.5545e-03,\n",
      "          1.3145e-03, -6.3793e-02,  9.2064e-03, -2.2487e-02, -3.8711e-02,\n",
      "          4.4274e-02, -4.5198e-02, -5.2146e-02, -1.9722e-02, -3.1566e-03,\n",
      "          6.4374e-02, -3.3794e-02, -1.7671e-02,  1.0079e-03,  2.9916e-02,\n",
      "         -5.2865e-02,  1.6794e-02, -4.7611e-02, -5.7812e-02, -2.4863e-02,\n",
      "          3.4989e-02,  3.8706e-03,  1.7247e-02, -3.3765e-02, -1.5472e-02,\n",
      "          1.4222e-02, -6.1199e-02,  2.2412e-02, -2.2962e-02,  3.0814e-02,\n",
      "          6.8993e-02, -1.6523e-02,  9.8808e-03, -6.3429e-02,  3.2641e-02,\n",
      "         -2.3346e-03,  1.8872e-02,  5.1447e-02,  6.0915e-02, -1.7272e-02,\n",
      "          2.6221e-02, -3.4509e-03, -2.6610e-02, -5.1863e-02, -4.1230e-02,\n",
      "          6.1108e-02, -1.5349e-03, -2.1992e-02,  3.4367e-02, -2.9099e-03,\n",
      "          1.4499e-02,  1.7564e-02, -6.7521e-02,  1.2467e-03, -2.8478e-02,\n",
      "          4.7494e-02, -2.4400e-02, -5.1284e-02, -2.5828e-02, -6.8766e-02,\n",
      "          3.2686e-02,  6.9519e-02, -1.2648e-03, -3.6621e-02,  2.1485e-02,\n",
      "          4.8003e-03,  6.1523e-02,  2.4813e-02, -8.1129e-03,  5.0403e-02,\n",
      "          5.2981e-02,  2.1440e-02, -1.0474e-02, -5.6044e-03,  2.2974e-02,\n",
      "          4.2686e-02,  6.2809e-02, -3.7588e-02, -5.0048e-02, -1.0602e-02,\n",
      "          4.5451e-02, -3.7596e-02, -2.4368e-02,  2.0073e-02,  3.6329e-02,\n",
      "          1.3395e-02, -5.8243e-02,  5.5343e-02,  3.2568e-02, -3.5433e-02,\n",
      "         -1.6735e-03,  2.7976e-02,  1.6454e-02,  3.1725e-02,  4.9984e-02,\n",
      "         -5.4298e-02, -1.4752e-02, -4.9618e-02, -1.2971e-02, -5.0865e-03,\n",
      "         -6.0458e-02, -7.9551e-03,  5.7786e-02, -3.2106e-02, -1.7834e-02,\n",
      "          6.5837e-02, -3.5728e-02,  2.7162e-02, -4.3608e-02,  1.6266e-02,\n",
      "         -2.2110e-03, -5.5979e-02,  5.9092e-02, -3.2625e-02, -6.0764e-02,\n",
      "         -3.7203e-02,  8.4929e-03, -3.5786e-02,  3.4598e-02,  6.2493e-02,\n",
      "         -1.5061e-03, -1.9153e-02,  2.3253e-02,  5.7481e-02,  4.8324e-02,\n",
      "          1.9751e-02,  3.6052e-02,  4.2621e-02,  5.2280e-02,  6.6904e-02,\n",
      "          2.8683e-02,  1.0379e-02, -1.0413e-02,  2.7859e-02, -1.4771e-02,\n",
      "          1.0640e-02, -5.7859e-02,  4.9119e-02, -3.8189e-02,  6.0461e-02,\n",
      "          2.2393e-02, -3.6557e-04, -5.0069e-03, -6.2893e-02,  6.6756e-02,\n",
      "         -2.8696e-02,  3.8250e-02,  6.6683e-02, -1.1364e-02, -3.6794e-02,\n",
      "          1.1923e-02,  1.8008e-02,  4.5520e-02, -3.4617e-02,  4.0030e-02,\n",
      "          3.6151e-03, -3.4264e-02,  1.7658e-02, -6.4058e-02,  2.6786e-02]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear2.bias | Size: torch.Size([124]) | Values : tensor([-0.0357,  0.0672], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in cnn_.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix model with data/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.crnn import CRNN\n",
    "from utils.utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetImg('data/2/img', 'data/2/label', 512, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=4,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data/char.txt'), 'r', encoding='utf-8') as f:\n",
    "    alphabet = f.read().rstrip()\n",
    "# print(alphabet)\n",
    "converter = strLabelConverter(alphabet, ignore_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(converter.numClass, 100, 0.1)\n",
    "\n",
    "criterion = torch.nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "checkpoint_path = 'pretrain/model_3.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn.conv0.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv0.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm0.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm0.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv1.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv1.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm1.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm1.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv2.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm2.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv3.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv3.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm3.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm3.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv4.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv4.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm4.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm4.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv5.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv5.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm5.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm5.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "linear1.weight tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "linear1.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.weight_ih_l0 tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.weight_hh_l0 tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.bias_ih_l0 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.bias_hh_l0 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.weight_ih_l0_reverse tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.weight_hh_l0_reverse tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.bias_ih_l0_reverse tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.bias_hh_l0_reverse tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "linear2.weight tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "linear2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(iter(dataloader))\n",
    "targets, target_lenghts = converter.encode(labels)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "preds = model(imgs)\n",
    "\n",
    "b, l, c = preds.shape\n",
    "preds_ = preds.permute(1, 0, 2)\n",
    "preds_lengths = torch.full(size=(b,), fill_value=l, dtype=torch.long).to('cpu')\n",
    "\n",
    "loss = criterion(preds_.log_softmax(2), targets, preds_lengths, target_lenghts) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(preds_.log_softmax(2), targets, preds_lengths, target_lenghts) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu ban đầu: [1, 2, 3]\n",
      "Dữ liệu sau khi thay đổi: [1, 2, 3, 4]\n",
      "Dữ liệu gốc cũng thay đổi: [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "class Example:\n",
    "    def __init__(self, list_data):\n",
    "        self.data = list_data\n",
    "    \n",
    "    def modify_data(self, new_item):\n",
    "        self.data.append(new_item)\n",
    "\n",
    "# Tạo một list\n",
    "my_list = [1, 2, 3]\n",
    "\n",
    "# Tạo một đối tượng của class Example, truyền my_list vào\n",
    "example_obj = Example(my_list)\n",
    "\n",
    "# In dữ liệu ban đầu\n",
    "print(\"Dữ liệu ban đầu:\", example_obj.data)\n",
    "\n",
    "# Sử dụng phương thức của class để thay đổi dữ liệu\n",
    "example_obj.modify_data(4)\n",
    "\n",
    "# In dữ liệu sau khi thay đổi\n",
    "print(\"Dữ liệu sau khi thay đổi:\", example_obj.data)\n",
    "print(\"Dữ liệu gốc cũng thay đổi:\", my_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tandat17z",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
