{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\HUST\\20232\\ML\\Project_OCR\\HandwritingRecognition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VGG_FeatureExtractor(nn.Module):\n",
    "    \"\"\" FeatureExtractor of CRNN (https://arxiv.org/pdf/1507.05717.pdf) \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=512):\n",
    "        super(VGG_FeatureExtractor, self).__init__()\n",
    "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
    "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
    "        self.ConvNet = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 64x16x50\n",
    "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 128x8x25\n",
    "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True)\n",
    "        )  # 512x1x24\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG_FeatureExtractor(1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512, 1, 199])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand((64, 1, 32, 800))\n",
    "output = vgg(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input : visual feature [batch_size x T x input_size]\n",
    "        output : contextual feature [batch_size x T x output_size]\n",
    "        \"\"\"\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
    "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CRNN_vgg(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class, input_channel, output_channel, hidden_size):\n",
    "        super(CRNN_vgg, self).__init__()\n",
    "        \"\"\" FeatureExtraction \"\"\"\n",
    "        self.FeatureExtraction = VGG_FeatureExtractor(input_channel, output_channel)\n",
    "        # elif opt.FeatureExtraction == 'RCNN':\n",
    "        #     self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        # elif opt.FeatureExtraction == 'ResNet':\n",
    "        #     self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        \n",
    "        self.FeatureExtraction_output = output_channel  # int(imgH/16-1) * 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
    "\n",
    "        \"\"\" Sequence modeling\"\"\"\n",
    "        self.SequenceModeling = nn.Sequential(\n",
    "                BidirectionalLSTM(self.FeatureExtraction_output, hidden_size, hidden_size),\n",
    "                BidirectionalLSTM(hidden_size, hidden_size, hidden_size))\n",
    "        self.SequenceModeling_output = hidden_size\n",
    "        \n",
    "        \"\"\" Prediction \"\"\"\n",
    "        self.Prediction = nn.Linear(self.SequenceModeling_output, num_class)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\" Feature extraction stage \"\"\"\n",
    "        visual_feature = self.FeatureExtraction(input)\n",
    "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
    "        visual_feature = visual_feature.squeeze(3)\n",
    "\n",
    "        \"\"\" Sequence modeling stage \"\"\"\n",
    "        contextual_feature = self.SequenceModeling(visual_feature)\n",
    "        \n",
    "        \"\"\" Prediction stage \"\"\"\n",
    "        prediction = self.Prediction(contextual_feature.contiguous())\n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(155, 1, 512, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 199, 155])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.crnn import CRNN\n",
    "from model.MyCrnn import MyCRNN\n",
    "from dataset import *\n",
    "from utils.StrLabelConverter import *\n",
    "\n",
    "import os\n",
    "\n",
    "from dataset_v2 import DatasetImg_v2\n",
    "from dataset import DatasetImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.dstype = 'v2'\n",
    "        self.root = 'data/data_v1'\n",
    "        self.imgW = 800\n",
    "        self.alphabet = 'data/char_v1.txt'\n",
    "        self.model = 'CRNN'\n",
    "        self.num_hidden = 200\n",
    "        self.dropout = 0.1\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 8\n",
    "        self.nepochs = 100\n",
    "        self.valInterval = 1\n",
    "        self.saveInterval = 1\n",
    "        self.savedir = 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Using cpu device\n",
      "---------------------------------------------------\n",
      "Sử dụng dataset_v2\n",
      "Num class:  154\n"
     ]
    }
   ],
   "source": [
    "device = ( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"Using {device} device\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "# --------------Tạo Dataset -------------------------------------------------------\n",
    "if opt.dstype == 'v1':\n",
    "    print('Sử dụng dataset_v1')\n",
    "    train_dataset = DatasetImg(opt.root + '/train/img', opt.root + '/train/label', imgW=opt.imgW)\n",
    "    test_dataset = DatasetImg(opt.root + '/test/img', opt.root + '/test/label', imgW=opt.imgW)\n",
    "elif opt.dstype == 'v2':\n",
    "    print('Sử dụng dataset_v2')\n",
    "    train_dataset = DatasetImg_v2(opt.root + '/train/img', opt.root + '/train/label')\n",
    "    test_dataset = DatasetImg_v2(opt.root + '/test/img', opt.root + '/test/label')\n",
    "\n",
    "with open(os.path.join(opt.alphabet), 'r', encoding='utf-8') as f:\n",
    "    alphabet = f.read().rstrip()\n",
    "\n",
    "converter = StrLabelConverter(alphabet)\n",
    "print('Num class: ', converter.numClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(64))\n",
    "subset = torch.utils.data.Subset(train_dataset, indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(subset,\n",
    "                    batch_size=16,\n",
    "                    shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> use CRNN-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if opt.model == 'MyCRNN':\n",
    "    model = MyCRNN(converter.numClass, opt.num_hidden, opt.dropout).to(device)\n",
    "elif opt.model == 'CRNN':\n",
    "    model = CRNN(converter.numClass, opt.num_hidden).to(device)\n",
    "\n",
    "criterion = torch.nn.CTCLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b152ad3010>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42) # Comment lại để cho khởi tạo tham số ngẫu nhiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> use CRNN-------------\n",
      "\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:08,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UUUUUU', 'UUUUUUUUUUUUU', 'UUUUU', 'UUUUU', 'UUUUUUUUUUUUUU', 'UUU', 'UUUUUUUUUU', 'UU', 'UUUUUUUUUUUU', 'UUUUUUUU', 'UUUUUUUUUU', 'UUUUU', 'UUUUUUUUUUU', 'UUUUUUUUUUU', 'UUUU', 'UU']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:05<00:05,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:08<00:02,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [1/101]\t avg_Loss/batch = 5.5638 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:05<00:05,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:08<00:02,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [2/101]\t avg_Loss/batch = 5.2982 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:05<00:05,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [3/101]\t avg_Loss/batch = 4.8580 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:05<00:05,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:08<00:03,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [4/101]\t avg_Loss/batch = 4.1880 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:06<00:06,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:10<00:03,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [5/101]\t avg_Loss/batch = 3.2643 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:06<00:06,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [6/101]\t avg_Loss/batch = 2.4548 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:06<00:06,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [7/101]\t avg_Loss/batch = 2.2036 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:06<00:06,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Epoch: [8/101]\t avg_Loss/batch = 2.2575 \t Levenshtein_Loss/sentence = 73.61\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:11,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:07<00:07,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:11<00:11,  5.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(loss)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss value is NaN or Inf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Backward -------------------------------------------------\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\tandat17z\\anaconda3\\envs\\env_tandat17z\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tandat17z\\anaconda3\\envs\\env_tandat17z\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.utils import GetInputCTCLoss\n",
    "model = CRNN(converter.numClass, opt.num_hidden).to(device)\n",
    "\n",
    "criterion = torch.nn.CTCLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "log = []\n",
    "# Training----------------------------------------------------------------------------------\n",
    "for epoch in range(1, 10 ):\n",
    "    print('Epoch: ', epoch)\n",
    "    # Train -------------------------\n",
    "    model.train(True)\n",
    "    total_loss = 0\n",
    "    levenshtein_loss = 0\n",
    "\n",
    "    t = tqdm(iter(train_dataloader), total=len(train_dataloader))\n",
    "    for batch_idx, (imgs, labels) in enumerate(t):\n",
    "        params_list = list(model.parameters())\n",
    "        log.append(params_list)\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        \n",
    "        preds_, preds_lengths, targets, target_lengths = GetInputCTCLoss(converter, preds, labels)\n",
    "        loss = criterion(preds_.log_softmax(2), targets, preds_lengths, target_lengths) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "        assert (not torch.isnan(loss) and not torch.isinf(loss)), \"Loss value is NaN or Inf\"\n",
    "        \n",
    "        # Backward -------------------------------------------------\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        _, enc_preds = preds.max(2)\n",
    "        sim_preds = converter.decode(enc_preds.view(-1), preds_lengths, raw = False)\n",
    "        print(sim_preds)\n",
    "        levenshtein_loss += converter.Levenshtein_loss(sim_preds, labels)\n",
    "\n",
    "    total_loss = total_loss/train_dataloader.sampler.num_samples *opt.batch_size\n",
    "    levenshtein_loss = levenshtein_loss/train_dataloader.sampler.num_samples \n",
    "    print('Epoch: [{}/{}]\\t avg_Loss/batch = {:.4f} \\t Levenshtein_Loss/sentence = {:.2f}'.format(epoch, 1 + opt.nepochs, total_loss, levenshtein_loss))\n",
    "    # log.append({\n",
    "    #     'type': 'train',\n",
    "    #     'epoch': epoch,\n",
    "    #     'avg_Loss': total_loss,\n",
    "    #     'levenshtein_Loss': levenshtein_loss\n",
    "    # })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0039, 0.0000, 0.0157,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ỨậỨĂỨậỨậỨ0', 'ỨậỨ0', 'ỨậỨ0', 'ỨậỨ0', 'Ứ0', 'Ứ0', 'ỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨ0', 'Ứ0', 'Ứ0', 'ỨậĂỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậ.ỨậỨậĂỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'ỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậĂậỨậỨĂậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨĂỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨ0', 'ỨậỨậỨ0', 'ỨậỨậĂỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'ỨậỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨ0', 'ỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨ0', 'Ứ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨậỨậỨ0', 'ỨậỨậỨậỨ0', 'ỨậĂậỨậỨĂỨậĂậĂỨậỨậỨĂậỨậỨ0', 'ỨậỨậỨậỨ0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "\n",
    "with torch.no_grad():\n",
    "    avg_loss = 0\n",
    "    avg_levenshtein_loss = 0\n",
    "\n",
    "    t = tqdm(iter(trainer.test_dataloader), total=len(trainer.test_dataloader))\n",
    "    for batch_idx, (imgs, labels) in enumerate(t):\n",
    "        imgs = imgs.to(trainer.device)\n",
    "        preds = model(imgs)\n",
    "        print(imgs)\n",
    "        # Compute Loss -------------------------------------------\n",
    "        preds_, preds_lengths, targets, target_lengths = GetInputCTCLoss(trainer.converter, preds, labels)\n",
    "        loss = trainer.criterion(preds_.log_softmax(2), targets, preds_lengths, target_lengths) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "        avg_loss += loss.detach().item()\n",
    "\n",
    "        _, enc_preds = preds.max(2)\n",
    "        sim_preds = trainer.converter.decode(enc_preds.view(-1), preds_lengths, raw = False)\n",
    "        avg_levenshtein_loss += trainer.converter.Levenshtein_loss(sim_preds, labels)\n",
    "        print(sim_preds)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Số 1 Phạm Hùng, Phường Dịch Vọng Hậu, Quận Cầu Giấy, Hà Nội',\n",
       " '45 Đường Số 27 P.Tân Phong, Phường Tân Phong, Quận 7, TP Hồ Chí Minh',\n",
       " '9/9 (644), quốc lộ 1A, P4, Phường 4, Thành Phố Tân An, Long An',\n",
       " '316 Lê Văn Sỹ, Phường 1, Quận Tân Bình, TP Hồ Chí Minh',\n",
       " 'Xã Quảng Hùng, Huyện Quảng Xương, Thanh Hoá',\n",
       " 'Số 29 Đường 320 Bông Sao, Phường 5, Quận 8, TP Hồ Chí Minh',\n",
       " '64/31 Nguyên Hồng, Phường 1, Quận Gò Vấp, TP Hồ Chí Minh',\n",
       " 'Bình Thắng, Xã Bình Thắng, Huyện Bù Gia Mập, Bình Phước',\n",
       " '387 Đào Trí, Phường Phú Thuận, Quận 7, TP Hồ Chí Minh',\n",
       " 'Thôn Thượng Giang, Xã Tây Giang, Huyện Tây Sơn, Bình Định',\n",
       " '375 Thuỵ Khuê, phường Bưởi, Quận Tây Hồ, Hà Nội',\n",
       " 'Số 459, phố Kim Ngưu, Phường Vĩnh Tuy, Quận Hai Bà Trưng, Hà Nội',\n",
       " 'ấp Chánh, Xã Đức Lập Hạ, Huyện Đức Hoà, Long An',\n",
       " 'Số 04 Lê Phụng Hiểu, Phường Lam Sơn, Thành Phố Thanh Hoá, Thanh Hoá',\n",
       " '95A Nguyễn Du, Thị trấn Ea Súp, Huyện Ea Súp, Đắc Lắc',\n",
       " '121B Trần Nhật Duật, Phường Phước Hòa, Thành phố Nha Trang, Khánh Hòa',\n",
       " '384 Hoàng Diệu, phường 06, Quận 4, TP Hồ Chí Minh',\n",
       " 'Số 57/22 Phú Lộc, Phường 6, Quận Tân Bình, TP Hồ Chí Minh',\n",
       " 'Thôn Văn Yên, Xã Ia Yok, Huyện Ia Grai, Gia Lai',\n",
       " 'Số 35-37-39, Nguyễn Huệ, Phường 1, Thành phố Cao Lãnh, Đồng Tháp',\n",
       " 'Thôn Yên Trung, Xã Yên Bình, Huyện Quang Bình, Hà Giang',\n",
       " 'Tổ dân phố 4, Thị trấn Đức Phổ, Huyện Đức Phổ, Quảng Ngãi',\n",
       " 'Số 15 Trần Hưng Đạo, Phường Phan Chu Trinh, Quận Hoàn Kiếm, Hà Nội',\n",
       " '15 Cây Keo, Phường Hiệp Tân, Quận Tân phú, TP Hồ Chí Minh',\n",
       " 'Lầu 2, 52 Trần Văn Danh, Phường 13, Quận Tân Bình, TP Hồ Chí Minh',\n",
       " 'Bản Hoàng Mã, Xã Chiềng Khoong, Huyện Sông Mã, Sơn La',\n",
       " '487A/8 KV Bình Yên A, P Long Hòa, Quận Bình Thuỷ, Cần Thơ',\n",
       " 'Khóm 6, Thị Trấn Sông Đốc, Huyện Trần Văn Thời, Cà Mau',\n",
       " 'Tổ 4, ấp Xóm Gò, Xã Long Phước, Huyện Long Thành, Đồng Nai',\n",
       " '42-44, ấp Vĩnh Tiến, Thị Trấn Vĩnh Thạnh, Huyện Vĩnh Thạnh, Cần Thơ',\n",
       " '295A Bùi Minh Trực, Phường 6, Quận 8, TP Hồ Chí Minh',\n",
       " 'Số 10, đường Phổ Quang, Phường 2, Quận Tân Bình, TP Hồ Chí Minh',\n",
       " '48/6 Hồ Biểu Chánh, Phường 11, Quận Phú Nhuận, TP Hồ Chí Minh',\n",
       " 'Số 137 đường Phan Chu Trinh, Phường Đội Cung, Thành phố Vinh, Nghệ An',\n",
       " 'Thôn Kìm, xã Vũ Lạc,, Thành phố Thái Bình, Thái Bình',\n",
       " 'Khu đô thị Tp Lễ Hội, Phường Vĩnh Mỹ, Thành phố Châu Đốc, An Giang',\n",
       " 'Thôn Ngãi Cầu, Xã An Khánh, Huyện Hoài Đức, Hà Nội',\n",
       " '27 Bàu Tràm 2, Phường Khuê Trung, Quận Cẩm Lệ, Đà Nẵng',\n",
       " 'Tổ 5, Phường Tân Phong, Thành Phố Lai Châu, Lai Châu',\n",
       " 'xã Quách Phẩm Bắc, Xã Quách Phẩm Bắc, Huyện Đầm Dơi, Cà Mau',\n",
       " 'Tổ 6, thị trấn Yên Minh, Thị trấn Yên Minh, Huyện Yên Minh, Hà Giang',\n",
       " 'Số 24 Đồng Tâm, Phường Đồng Hoà, Quận Kiến An, Hải Phòng',\n",
       " 'Vinh Hưng, Huyện Phú Lộc, Thừa Thiên - Huế',\n",
       " '93/4 Nguyễn Phúc Chu, Phường 15, Quận Tân Bình, TP Hồ Chí Minh',\n",
       " '59 Đường số 9, Cư xá Đài Ra Đa, Phường 13, Quận 6, TP Hồ Chí Minh',\n",
       " 'Thôn Yên Viên, Xã Vân Hà, Huyện Việt Yên, Bắc Giang',\n",
       " '26/24 Nguyễn Bỉnh Khiêm, Phường Đa Kao, Quận 1, TP Hồ Chí Minh',\n",
       " 'Số nhà 112, Phường Cao Thắng, Thành phố Hạ Long, Quảng Ninh',\n",
       " '327 Đường 23/10, Phường Ngọc Hiệp, Thành phố Nha Trang, Khánh Hòa',\n",
       " 'Thôn Chánh Giáo, Xã Mỹ An, Huyện Phù Mỹ, Bình Định',\n",
       " 'B16/14, ấp 2, Xã Bình Chánh, Huyện Bình Chánh, TP Hồ Chí Minh',\n",
       " '40 Bà Huyện Thanh Quan, Quận 3, TP Hồ Chí Minh',\n",
       " '391 Giải Phóng, Thị trấn Phước An, Huyện Krông Pắk, Đắc Lắc',\n",
       " 'TT Quán Hàu, Huyện Quảng Ninh, Quảng Bình',\n",
       " 'Thôn Yên Hòa, Xã Đa Lộc, Huyện Hậu Lộc, Thanh Hoá',\n",
       " '68 Đặng Dung, phường Thuận Thành, Thành phố Huế, Thừa Thiên - Huế',\n",
       " 'Tổ 2, Phường Ngọc Hà, TP Hà Giang, Hà Giang',\n",
       " '53C Hồ Hảo Hớn, Phường Cô Giang, Quận 1, TP Hồ Chí Minh',\n",
       " '84/14A Phạm Hùng, Xã Bình Hưng, Huyện Bình Chánh, TP Hồ Chí Minh',\n",
       " '32/17 Phan Đăng Lưu, Phường Hoà Cường Bắc, Quận Hải Châu, Đà Nẵng',\n",
       " 'Thị trấn Uyên hưng, Thị xã Tân Uyên, Bình Dương',\n",
       " 'Tổ 22, Phường Pú Trạng, Thị xã Nghĩa Lộ, Yên Bái',\n",
       " '299 - đường Trần Đăng Ninh, Thành Phố Lạng Sơn, Lạng Sơn',\n",
       " '39/38A Chánh Hưng, Phường 10, Quận 8, TP Hồ Chí Minh')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(imgs[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, nclass, num_hidden, dropout = 0):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        ks = [ 3,   3,   3,   3,   3,   3,   1,   1,   1]\n",
    "        ss = [ 1,   1,   1,   1,   1,   1,   1,   1,   1]\n",
    "        ps = [ 1,   1,   1,   1,   1,   1,   1,   1,   1]\n",
    "        nm = [64, 128, 256, 256, 256, 512, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "        def convRelu(i):\n",
    "            nIn = 1 if i == 0 else nm[i - 1] \n",
    "            nOut = nm[i]\n",
    "            # cnn.add_module('conv{0}'.format(i),\n",
    "            #                nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            cnn.add_module('conv{0}'.format(i), nn.Conv2d(nIn, nOut, 3, 1, 1))\n",
    "            cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        # input : (C, H, W) - (1, 32, 512)\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d((2, 2)))  # 64, 16, 256\n",
    "        convRelu(1) \n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d((2, 2)))  # 128, 8, 128\n",
    "        convRelu(2) \n",
    "        convRelu(3) \n",
    "        cnn.add_module('pooling{0}'.format(2), nn.MaxPool2d((2, 1)))  # 256, 4, 128\n",
    "        convRelu(4) \n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d((2, 1)))  # 512, 2, 128\n",
    "        convRelu(6) \n",
    "        convRelu(7)\n",
    "        cnn.add_module('pooling{0}'.format(4), nn.MaxPool2d((2, 1)))  # 512, 1, 128\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        # BiLSTM\n",
    "        self.biLSTM1 = nn.LSTM(512, num_hidden, bidirectional=True, batch_first = True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(num_hidden * 2, nclass, bias = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        x1 = self.cnn(input) # b, 512, 1, 128\n",
    "        x1 = torch.squeeze(x1, 2) # b, 512, 128\n",
    "        x1 = x1.permute(0, 2, 1)  # b, 128, 512\n",
    "\n",
    "        x2, _  = self.biLSTM1(x1) # b, 128, num_hidden*2\n",
    "        x2 = self.dropout1(x2) \n",
    "\n",
    "        x3 = self.linear(x2) # b, 128, num_class\n",
    "        out = self.dropout(x3)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 124])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_ = CRNN(124, 100)\n",
    "out = cnn_(torch.rand(64, 1, 32, 512))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "    (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu7): ReLU(inplace=True)\n",
       "    (pooling4): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (biLSTM1): LSTM(512, 100, batch_first=True, bidirectional=True)\n",
       "  (dropout1): Dropout(p=0, inplace=False)\n",
       "  (linear): Linear(in_features=200, out_features=124, bias=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: cnn.conv0.weight | Size: torch.Size([64, 1, 3, 3]) | Values : tensor([[[[ 0.1159, -0.0922, -0.0774],\n",
      "          [-0.0084, -0.2158,  0.0290],\n",
      "          [-0.3093,  0.2286, -0.3006]]],\n",
      "\n",
      "\n",
      "        [[[-0.0702,  0.1410, -0.0326],\n",
      "          [ 0.2566,  0.0519,  0.0762],\n",
      "          [-0.1566,  0.0275, -0.0293]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv0.bias | Size: torch.Size([64]) | Values : tensor([-0.2395,  0.2840], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm0.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm0.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv1.weight | Size: torch.Size([128, 64, 3, 3]) | Values : tensor([[[[ 0.0385, -0.0375, -0.0349],\n",
      "          [ 0.0327, -0.0155, -0.0409],\n",
      "          [-0.0165, -0.0222, -0.0108]],\n",
      "\n",
      "         [[ 0.0355,  0.0303,  0.0415],\n",
      "          [ 0.0296,  0.0169, -0.0204],\n",
      "          [ 0.0227, -0.0389,  0.0119]],\n",
      "\n",
      "         [[ 0.0374,  0.0096,  0.0357],\n",
      "          [ 0.0135,  0.0097,  0.0378],\n",
      "          [ 0.0343, -0.0051, -0.0267]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0257,  0.0380,  0.0095],\n",
      "          [-0.0335, -0.0186,  0.0248],\n",
      "          [-0.0400, -0.0254, -0.0192]],\n",
      "\n",
      "         [[ 0.0405,  0.0013, -0.0178],\n",
      "          [-0.0173, -0.0323,  0.0231],\n",
      "          [ 0.0197, -0.0399,  0.0352]],\n",
      "\n",
      "         [[ 0.0007, -0.0020,  0.0003],\n",
      "          [ 0.0327, -0.0329, -0.0411],\n",
      "          [ 0.0362, -0.0216,  0.0298]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036,  0.0176, -0.0133],\n",
      "          [ 0.0205, -0.0349, -0.0380],\n",
      "          [-0.0075,  0.0291, -0.0072]],\n",
      "\n",
      "         [[-0.0186, -0.0056,  0.0024],\n",
      "          [ 0.0256,  0.0100,  0.0058],\n",
      "          [ 0.0290,  0.0102, -0.0272]],\n",
      "\n",
      "         [[ 0.0072,  0.0361,  0.0387],\n",
      "          [ 0.0058, -0.0093,  0.0123],\n",
      "          [-0.0348,  0.0025,  0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0157, -0.0174, -0.0367],\n",
      "          [ 0.0080, -0.0195, -0.0207],\n",
      "          [ 0.0174, -0.0170, -0.0235]],\n",
      "\n",
      "         [[ 0.0121, -0.0127, -0.0076],\n",
      "          [ 0.0053,  0.0260,  0.0109],\n",
      "          [ 0.0176,  0.0096,  0.0359]],\n",
      "\n",
      "         [[-0.0012,  0.0367, -0.0341],\n",
      "          [-0.0163,  0.0162, -0.0009],\n",
      "          [ 0.0313, -0.0364, -0.0193]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv1.bias | Size: torch.Size([128]) | Values : tensor([-0.0359,  0.0341], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm1.weight | Size: torch.Size([128]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm1.bias | Size: torch.Size([128]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv2.weight | Size: torch.Size([128, 128, 3, 3]) | Values : tensor([[[[ 0.0098,  0.0174,  0.0048],\n",
      "          [-0.0120, -0.0247, -0.0219],\n",
      "          [-0.0033,  0.0280, -0.0100]],\n",
      "\n",
      "         [[ 0.0082,  0.0228,  0.0015],\n",
      "          [-0.0134, -0.0190, -0.0075],\n",
      "          [ 0.0007, -0.0150, -0.0045]],\n",
      "\n",
      "         [[ 0.0084, -0.0049, -0.0206],\n",
      "          [ 0.0072, -0.0214,  0.0231],\n",
      "          [-0.0252, -0.0180, -0.0162]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0130, -0.0090,  0.0201],\n",
      "          [ 0.0074, -0.0126, -0.0105],\n",
      "          [-0.0255,  0.0156, -0.0175]],\n",
      "\n",
      "         [[-0.0234,  0.0075, -0.0225],\n",
      "          [ 0.0205, -0.0156,  0.0198],\n",
      "          [ 0.0164,  0.0091,  0.0108]],\n",
      "\n",
      "         [[ 0.0097, -0.0265, -0.0118],\n",
      "          [ 0.0141,  0.0176, -0.0236],\n",
      "          [-0.0078,  0.0271,  0.0150]]],\n",
      "\n",
      "\n",
      "        [[[-0.0235, -0.0273,  0.0228],\n",
      "          [ 0.0233,  0.0124, -0.0055],\n",
      "          [ 0.0156, -0.0110, -0.0285]],\n",
      "\n",
      "         [[-0.0108,  0.0096, -0.0257],\n",
      "          [-0.0013,  0.0268,  0.0238],\n",
      "          [-0.0073, -0.0053, -0.0178]],\n",
      "\n",
      "         [[ 0.0011,  0.0244, -0.0116],\n",
      "          [-0.0090,  0.0141, -0.0154],\n",
      "          [-0.0192, -0.0090, -0.0237]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0257,  0.0001,  0.0014],\n",
      "          [ 0.0229,  0.0225, -0.0097],\n",
      "          [ 0.0146, -0.0269,  0.0203]],\n",
      "\n",
      "         [[-0.0192,  0.0053, -0.0220],\n",
      "          [ 0.0268, -0.0180,  0.0255],\n",
      "          [-0.0033, -0.0226, -0.0103]],\n",
      "\n",
      "         [[-0.0048, -0.0287,  0.0075],\n",
      "          [ 0.0095,  0.0224, -0.0064],\n",
      "          [-0.0168, -0.0145, -0.0223]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv2.bias | Size: torch.Size([128]) | Values : tensor([ 0.0017, -0.0026], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm2.weight | Size: torch.Size([128]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm2.bias | Size: torch.Size([128]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv3.weight | Size: torch.Size([256, 128, 3, 3]) | Values : tensor([[[[ 0.0140, -0.0269, -0.0153],\n",
      "          [-0.0253,  0.0171, -0.0082],\n",
      "          [ 0.0069, -0.0256, -0.0132]],\n",
      "\n",
      "         [[ 0.0273, -0.0102,  0.0033],\n",
      "          [ 0.0058, -0.0190, -0.0011],\n",
      "          [ 0.0051,  0.0185,  0.0091]],\n",
      "\n",
      "         [[ 0.0191,  0.0046,  0.0185],\n",
      "          [ 0.0142,  0.0032,  0.0225],\n",
      "          [-0.0167,  0.0232, -0.0195]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0256, -0.0243,  0.0115],\n",
      "          [-0.0009,  0.0233, -0.0155],\n",
      "          [ 0.0080,  0.0114,  0.0024]],\n",
      "\n",
      "         [[-0.0026,  0.0073,  0.0071],\n",
      "          [-0.0173, -0.0089, -0.0252],\n",
      "          [-0.0144,  0.0164,  0.0093]],\n",
      "\n",
      "         [[-0.0264, -0.0162, -0.0069],\n",
      "          [ 0.0069, -0.0121, -0.0179],\n",
      "          [ 0.0192,  0.0196, -0.0139]]],\n",
      "\n",
      "\n",
      "        [[[-0.0185,  0.0185,  0.0216],\n",
      "          [-0.0155,  0.0124, -0.0288],\n",
      "          [-0.0179,  0.0205, -0.0221]],\n",
      "\n",
      "         [[-0.0084, -0.0045,  0.0261],\n",
      "          [ 0.0170,  0.0220,  0.0036],\n",
      "          [ 0.0022, -0.0288,  0.0125]],\n",
      "\n",
      "         [[ 0.0099, -0.0164, -0.0292],\n",
      "          [-0.0003,  0.0025,  0.0280],\n",
      "          [ 0.0215,  0.0006, -0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0037,  0.0150, -0.0075],\n",
      "          [ 0.0276,  0.0182, -0.0225],\n",
      "          [ 0.0062, -0.0259, -0.0258]],\n",
      "\n",
      "         [[-0.0238,  0.0285, -0.0181],\n",
      "          [-0.0062, -0.0128, -0.0252],\n",
      "          [ 0.0055,  0.0217,  0.0282]],\n",
      "\n",
      "         [[-0.0035,  0.0067,  0.0043],\n",
      "          [-0.0238, -0.0190,  0.0015],\n",
      "          [-0.0012, -0.0242, -0.0162]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv3.bias | Size: torch.Size([256]) | Values : tensor([-0.0161, -0.0055], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm3.weight | Size: torch.Size([256]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm3.bias | Size: torch.Size([256]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv4.weight | Size: torch.Size([256, 256, 3, 3]) | Values : tensor([[[[ 0.0012, -0.0133,  0.0112],\n",
      "          [-0.0016,  0.0083, -0.0054],\n",
      "          [ 0.0045, -0.0115,  0.0093]],\n",
      "\n",
      "         [[ 0.0032, -0.0165, -0.0177],\n",
      "          [ 0.0038, -0.0063, -0.0102],\n",
      "          [-0.0149, -0.0179, -0.0068]],\n",
      "\n",
      "         [[ 0.0043,  0.0086, -0.0098],\n",
      "          [ 0.0169,  0.0167,  0.0130],\n",
      "          [-0.0127, -0.0174, -0.0130]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0184, -0.0034, -0.0034],\n",
      "          [-0.0143,  0.0045, -0.0070],\n",
      "          [ 0.0146,  0.0090, -0.0088]],\n",
      "\n",
      "         [[ 0.0082, -0.0172, -0.0188],\n",
      "          [-0.0029, -0.0120,  0.0207],\n",
      "          [ 0.0071,  0.0120,  0.0092]],\n",
      "\n",
      "         [[ 0.0110, -0.0095, -0.0079],\n",
      "          [-0.0058,  0.0062,  0.0093],\n",
      "          [ 0.0178,  0.0163, -0.0009]]],\n",
      "\n",
      "\n",
      "        [[[-0.0057, -0.0050,  0.0161],\n",
      "          [-0.0173, -0.0003, -0.0154],\n",
      "          [ 0.0097,  0.0189,  0.0151]],\n",
      "\n",
      "         [[-0.0074,  0.0169, -0.0039],\n",
      "          [-0.0111, -0.0194, -0.0167],\n",
      "          [-0.0129,  0.0127,  0.0084]],\n",
      "\n",
      "         [[-0.0041,  0.0122,  0.0185],\n",
      "          [ 0.0139, -0.0091,  0.0056],\n",
      "          [-0.0183, -0.0022,  0.0143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0023, -0.0145, -0.0039],\n",
      "          [-0.0088,  0.0089, -0.0062],\n",
      "          [ 0.0037,  0.0120,  0.0081]],\n",
      "\n",
      "         [[-0.0032,  0.0156, -0.0035],\n",
      "          [ 0.0162,  0.0073,  0.0097],\n",
      "          [ 0.0197,  0.0086,  0.0035]],\n",
      "\n",
      "         [[ 0.0138,  0.0152, -0.0062],\n",
      "          [ 0.0102,  0.0185,  0.0100],\n",
      "          [-0.0188, -0.0055, -0.0170]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv4.bias | Size: torch.Size([256]) | Values : tensor([-0.0045,  0.0196], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm4.weight | Size: torch.Size([256]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm4.bias | Size: torch.Size([256]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv5.weight | Size: torch.Size([512, 256, 3, 3]) | Values : tensor([[[[-5.2392e-04,  1.4282e-02,  1.7927e-02],\n",
      "          [-1.8987e-02, -1.1316e-02, -1.3873e-02],\n",
      "          [ 1.1543e-02, -1.4394e-02, -2.0237e-02]],\n",
      "\n",
      "         [[-1.3556e-02,  1.2339e-02, -9.5207e-03],\n",
      "          [ 3.1045e-03,  5.4448e-03, -1.1988e-02],\n",
      "          [ 1.9063e-02, -1.9103e-03,  9.9915e-03]],\n",
      "\n",
      "         [[-5.2700e-03, -1.6454e-02,  4.0227e-03],\n",
      "          [-3.5082e-03,  1.1289e-02, -7.9002e-04],\n",
      "          [ 8.4851e-04,  1.8034e-02,  1.4488e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0056e-02,  2.9534e-03, -4.4502e-03],\n",
      "          [-1.0878e-02,  1.9090e-02,  1.5800e-03],\n",
      "          [-1.6499e-03, -1.4303e-02,  2.0577e-02]],\n",
      "\n",
      "         [[-1.9564e-02,  4.5523e-03, -9.2791e-03],\n",
      "          [-3.8313e-03, -6.3101e-03,  1.9779e-02],\n",
      "          [ 7.5752e-03, -1.2657e-02, -2.0309e-04]],\n",
      "\n",
      "         [[ 3.9003e-04,  3.5831e-03, -1.2742e-02],\n",
      "          [ 1.8475e-02, -3.5799e-03,  1.0175e-03],\n",
      "          [-9.8513e-05,  1.8291e-02, -8.2514e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.7962e-02,  4.9331e-03,  1.9971e-02],\n",
      "          [-2.0400e-02, -1.1543e-02,  1.7658e-02],\n",
      "          [ 4.6525e-03,  4.6134e-03, -1.4979e-02]],\n",
      "\n",
      "         [[ 4.7038e-03, -1.5978e-02,  1.9651e-02],\n",
      "          [ 1.8260e-02, -1.6414e-02,  1.7017e-02],\n",
      "          [ 3.1004e-04,  5.4100e-03, -2.0613e-02]],\n",
      "\n",
      "         [[-9.4370e-03,  1.3011e-02,  1.4146e-02],\n",
      "          [-2.0785e-03,  1.8565e-02,  1.8563e-02],\n",
      "          [-2.9047e-03,  1.1311e-02, -1.3513e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5290e-05, -8.1031e-03, -1.1413e-02],\n",
      "          [-1.1337e-02,  1.3682e-02,  1.5184e-02],\n",
      "          [-1.3299e-03, -3.4721e-03,  1.4401e-02]],\n",
      "\n",
      "         [[ 1.2008e-02, -1.4137e-02, -1.4221e-02],\n",
      "          [-1.0214e-02,  1.4263e-02,  1.8432e-02],\n",
      "          [-1.4819e-02, -1.1264e-02,  7.6885e-03]],\n",
      "\n",
      "         [[-1.7166e-02, -7.7484e-03, -1.6840e-02],\n",
      "          [-9.8598e-03,  1.1854e-02, -1.3389e-02],\n",
      "          [ 9.3272e-03,  1.2323e-02, -1.2469e-02]]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.conv5.bias | Size: torch.Size([512]) | Values : tensor([9.7811e-05, 1.7900e-02], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm5.weight | Size: torch.Size([512]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: cnn.batchnorm5.bias | Size: torch.Size([512]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear1.weight | Size: torch.Size([128, 64]) | Values : tensor([[-0.0406, -0.1229,  0.0023, -0.0873, -0.0873,  0.0666, -0.0134, -0.0587,\n",
      "          0.0211,  0.1031,  0.0113,  0.0963,  0.0513,  0.0843,  0.1247, -0.0459,\n",
      "          0.0632, -0.0901,  0.0033,  0.0880, -0.0596, -0.0082,  0.0088,  0.0142,\n",
      "         -0.0297,  0.1218,  0.0081, -0.1159, -0.0482,  0.0317, -0.1100, -0.0239,\n",
      "         -0.0597, -0.1041, -0.0801, -0.0891, -0.0551,  0.0079, -0.0093, -0.1186,\n",
      "         -0.0669, -0.0457, -0.0501, -0.1050, -0.0599, -0.0774,  0.0112,  0.0747,\n",
      "         -0.0619,  0.0588,  0.0915,  0.0913,  0.0111,  0.1239,  0.0878, -0.0267,\n",
      "         -0.0859, -0.0971,  0.1234, -0.0130, -0.0790, -0.0901, -0.0646,  0.0156],\n",
      "        [ 0.1112,  0.1172, -0.1067, -0.0032, -0.1031, -0.0554, -0.0841, -0.0787,\n",
      "         -0.0785, -0.0480,  0.0706,  0.1030,  0.0282, -0.0007, -0.1072, -0.0206,\n",
      "          0.0246,  0.0028, -0.0695, -0.1213,  0.0815, -0.0409,  0.1191, -0.0767,\n",
      "          0.0934,  0.0300,  0.1005, -0.0045, -0.0227, -0.1041, -0.0979, -0.1211,\n",
      "          0.0110,  0.0426,  0.0567, -0.1204, -0.1063,  0.0312, -0.0919, -0.0667,\n",
      "         -0.0158, -0.1037,  0.0821, -0.0629,  0.0009,  0.0855, -0.0243,  0.0224,\n",
      "         -0.0797,  0.0707,  0.0483,  0.0629,  0.0773,  0.0870, -0.0720, -0.0457,\n",
      "          0.0342,  0.0285,  0.0684,  0.0267, -0.0922,  0.0922,  0.0998, -0.0915]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear1.bias | Size: torch.Size([128]) | Values : tensor([0.0808, 0.0235], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_ih_l0 | Size: torch.Size([400, 512]) | Values : tensor([[-0.0843,  0.0494,  0.0253,  ..., -0.0349, -0.0668, -0.0133],\n",
      "        [-0.0922, -0.0750,  0.0565,  ...,  0.0884, -0.0987,  0.0407]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_hh_l0 | Size: torch.Size([400, 100]) | Values : tensor([[ 0.0123, -0.0297, -0.0457,  0.0953, -0.0260, -0.0597,  0.0792,  0.0800,\n",
      "          0.0013, -0.0095, -0.0840, -0.0578,  0.0473, -0.0190, -0.0417,  0.0234,\n",
      "         -0.0789, -0.0851,  0.0315,  0.0026, -0.0376,  0.0263,  0.0006, -0.0307,\n",
      "          0.0301, -0.0928, -0.0752, -0.0370, -0.0042, -0.0795, -0.0900, -0.0897,\n",
      "         -0.0760,  0.0588,  0.0398,  0.0767,  0.0128, -0.0806, -0.0769, -0.0790,\n",
      "          0.0967,  0.0896,  0.0647,  0.0352, -0.0574, -0.0585,  0.0272, -0.0449,\n",
      "          0.0327,  0.0477,  0.0889, -0.0355,  0.0786,  0.0209, -0.0046,  0.0332,\n",
      "          0.0848,  0.0757, -0.0927, -0.0561,  0.0153,  0.0222, -0.0058,  0.0987,\n",
      "         -0.0150, -0.0269, -0.0163,  0.0933, -0.0463,  0.0692, -0.0629, -0.0336,\n",
      "         -0.0209,  0.0337, -0.0698,  0.0684,  0.0038, -0.0716, -0.0202, -0.0516,\n",
      "         -0.0125,  0.0388,  0.0806, -0.0615,  0.0596,  0.0713, -0.0387, -0.0960,\n",
      "         -0.0935,  0.0669,  0.0652,  0.0477, -0.0086, -0.0156,  0.0023, -0.0830,\n",
      "          0.0985,  0.0767, -0.0696, -0.0886],\n",
      "        [ 0.0467,  0.0938, -0.0174, -0.0524,  0.0284,  0.0709, -0.0159,  0.0080,\n",
      "         -0.0863, -0.0433, -0.0211,  0.0247, -0.0410, -0.0437, -0.0946,  0.0361,\n",
      "          0.0526,  0.0219,  0.0066, -0.0775,  0.0335,  0.0924,  0.0320,  0.0176,\n",
      "          0.0158,  0.0886, -0.0033, -0.0793, -0.0761, -0.0067, -0.0948, -0.0287,\n",
      "         -0.0009,  0.0517, -0.0494, -0.0528, -0.0579,  0.0493,  0.0202, -0.0954,\n",
      "         -0.0849,  0.0208,  0.0111, -0.0791, -0.0712, -0.0323,  0.0276,  0.0975,\n",
      "          0.0639,  0.0060,  0.0403,  0.0711,  0.0605,  0.0888, -0.0715, -0.0398,\n",
      "         -0.0282, -0.0260, -0.0739,  0.0097, -0.0514,  0.0257,  0.0697, -0.0279,\n",
      "         -0.0441,  0.0633, -0.0949, -0.0374, -0.0422, -0.0362,  0.0236,  0.0900,\n",
      "          0.0597,  0.0167,  0.0626, -0.0990,  0.0086,  0.0209,  0.0121, -0.0217,\n",
      "         -0.0670, -0.0707, -0.0037, -0.0619,  0.0069, -0.0142,  0.0016,  0.0490,\n",
      "         -0.0257,  0.0434, -0.0973, -0.0083,  0.0574, -0.0563, -0.0197, -0.0685,\n",
      "         -0.0382, -0.0127, -0.0297,  0.0603]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_ih_l0 | Size: torch.Size([400]) | Values : tensor([-0.0405,  0.0041], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_hh_l0 | Size: torch.Size([400]) | Values : tensor([ 0.0247, -0.0325], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_ih_l0_reverse | Size: torch.Size([400, 512]) | Values : tensor([[ 0.0486,  0.0459, -0.0044,  ...,  0.0371, -0.0451,  0.0581],\n",
      "        [-0.0834,  0.0583,  0.0047,  ...,  0.0515,  0.0734,  0.0519]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.weight_hh_l0_reverse | Size: torch.Size([400, 100]) | Values : tensor([[-0.0380, -0.0633, -0.0883, -0.0190,  0.0357,  0.0547, -0.0580,  0.0596,\n",
      "         -0.0965,  0.0388, -0.0159,  0.0059, -0.0865, -0.0667, -0.0937, -0.0431,\n",
      "         -0.0364,  0.0664,  0.0887, -0.0091, -0.0261, -0.0469, -0.0393,  0.0526,\n",
      "         -0.0260,  0.0878, -0.0488, -0.0759,  0.0585, -0.0930, -0.0001, -0.0496,\n",
      "          0.0309, -0.0839, -0.0391, -0.0018, -0.0739, -0.0201,  0.0536, -0.0713,\n",
      "         -0.0354, -0.0250,  0.0581,  0.0278, -0.0883,  0.0150,  0.0439,  0.0994,\n",
      "          0.0679, -0.0992,  0.0071, -0.0332, -0.0964,  0.0752, -0.0557, -0.0830,\n",
      "          0.0141,  0.0531, -0.0343, -0.0437, -0.0870,  0.0592,  0.0074, -0.0036,\n",
      "          0.0959, -0.0600, -0.0432,  0.0497,  0.0386, -0.0884, -0.0304,  0.0862,\n",
      "         -0.0427, -0.0249,  0.0456,  0.0546, -0.0623,  0.0687, -0.0824,  0.0213,\n",
      "          0.0100,  0.0251,  0.0207,  0.0800,  0.0162,  0.0742,  0.0115, -0.0512,\n",
      "         -0.0419,  0.0335, -0.0934,  0.0077, -0.0987, -0.0089,  0.0531, -0.0066,\n",
      "          0.0871,  0.0036,  0.0157, -0.0597],\n",
      "        [-0.0774, -0.0009, -0.0813,  0.0535, -0.0235,  0.0454, -0.0586, -0.0745,\n",
      "         -0.0994,  0.0773,  0.0927, -0.0903, -0.0586, -0.0518, -0.0439, -0.0821,\n",
      "          0.0323,  0.0651,  0.0179,  0.0338,  0.0879, -0.0100,  0.0343,  0.0003,\n",
      "         -0.0564,  0.0091, -0.0344, -0.0848, -0.0225,  0.0393, -0.0685,  0.0584,\n",
      "         -0.0931,  0.0409,  0.0205,  0.0583, -0.0348,  0.0292,  0.0688, -0.0764,\n",
      "          0.0294, -0.0952,  0.0878, -0.0361, -0.0177, -0.0205,  0.0416,  0.0548,\n",
      "          0.0615, -0.0150, -0.0102,  0.0535, -0.0178,  0.0736,  0.0963,  0.0816,\n",
      "         -0.0844, -0.0129, -0.0725,  0.0700, -0.0011, -0.0469, -0.0150,  0.0078,\n",
      "          0.0877,  0.0467,  0.0501, -0.0158, -0.0008,  0.0625, -0.0290, -0.0516,\n",
      "         -0.0495,  0.0494,  0.0087, -0.0915,  0.0413,  0.0418, -0.0226,  0.0045,\n",
      "         -0.0069, -0.0623,  0.0139, -0.0182,  0.0225, -0.0036, -0.0089,  0.0207,\n",
      "          0.0610, -0.0479, -0.0822,  0.0766,  0.0302,  0.0669, -0.0108,  0.0261,\n",
      "         -0.0414,  0.0341,  0.0617, -0.0697]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_ih_l0_reverse | Size: torch.Size([400]) | Values : tensor([0.0368, 0.0252], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: biLSTM.bias_hh_l0_reverse | Size: torch.Size([400]) | Values : tensor([-0.0012, -0.0480], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear2.weight | Size: torch.Size([124, 200]) | Values : tensor([[ 2.9494e-02,  6.5491e-02, -5.7590e-02, -4.7205e-02, -5.3332e-02,\n",
      "         -1.8921e-02, -6.3015e-02,  5.8128e-02, -3.0939e-02,  3.1307e-02,\n",
      "          2.7958e-02,  1.4704e-02,  9.5869e-03, -7.1495e-03, -3.9242e-02,\n",
      "          1.0336e-02,  1.7041e-02, -3.3502e-02, -5.9951e-02, -5.4861e-02,\n",
      "         -5.6885e-02, -1.3631e-03, -3.4018e-02, -6.3891e-02, -6.8314e-02,\n",
      "         -4.5317e-02, -3.0445e-02,  2.3836e-03,  1.9910e-02, -1.9392e-02,\n",
      "         -2.6338e-02, -1.8305e-02, -4.0556e-02,  2.5726e-02,  5.7145e-02,\n",
      "          2.9603e-02, -2.2051e-02,  4.3138e-02,  6.6595e-02, -3.4166e-02,\n",
      "         -2.9343e-02, -7.0173e-02,  4.4687e-02,  3.7977e-02,  2.4513e-02,\n",
      "         -5.5577e-02, -6.9741e-02,  1.9391e-02,  2.3481e-02, -5.1319e-02,\n",
      "          3.6582e-02, -5.5459e-02, -2.8874e-02,  1.3981e-02, -5.1651e-03,\n",
      "          1.8653e-02,  8.2540e-03,  5.2821e-02, -2.2520e-02,  6.6553e-02,\n",
      "          3.9943e-02,  1.6818e-02, -5.1220e-03, -1.3070e-02, -2.8808e-02,\n",
      "          3.6909e-02,  6.5986e-02,  1.7970e-02, -1.4629e-02,  3.2585e-02,\n",
      "          6.0972e-02, -3.4600e-05,  6.5427e-02,  3.3894e-02, -5.6739e-02,\n",
      "         -3.2038e-02,  1.3067e-02, -1.3017e-02,  5.1911e-02,  5.0002e-02,\n",
      "          5.9934e-02,  2.5886e-02,  5.8120e-03, -1.3786e-02,  3.1384e-02,\n",
      "          5.5555e-03,  1.3490e-02, -2.3639e-02, -4.4613e-02, -5.7455e-02,\n",
      "         -3.8057e-02,  2.1600e-02, -4.5314e-02,  5.7062e-02, -3.5031e-02,\n",
      "         -3.2884e-02,  2.0577e-02, -2.7701e-02, -6.3263e-05,  5.3310e-02,\n",
      "         -4.2577e-02, -3.5264e-02,  6.0446e-03,  5.2341e-02,  2.7950e-02,\n",
      "          3.6590e-02,  2.8273e-02, -5.3243e-02, -1.9850e-02, -2.2753e-02,\n",
      "         -6.6494e-02,  1.8394e-04,  1.5386e-02, -4.6621e-02, -1.7630e-02,\n",
      "         -5.3794e-02, -1.3127e-02, -1.5287e-02, -6.2072e-02, -1.5861e-02,\n",
      "          3.7509e-02, -5.0312e-02, -1.4645e-02,  3.5279e-02,  5.4137e-02,\n",
      "         -1.6853e-02,  4.6877e-02, -6.1983e-02, -1.8474e-02,  5.8453e-02,\n",
      "         -6.5283e-03, -3.1387e-02,  5.8944e-02, -4.0292e-02, -4.3028e-02,\n",
      "          3.1448e-02,  3.3784e-02, -2.5220e-02,  6.4743e-02, -1.7155e-02,\n",
      "          5.7514e-02,  3.1684e-02,  2.4706e-02, -6.8471e-03, -5.6142e-02,\n",
      "          3.8959e-02,  2.8065e-02,  4.1284e-02,  3.9892e-02,  4.6422e-02,\n",
      "          4.6289e-02,  1.9727e-02,  3.0843e-02, -6.2048e-02,  4.7252e-02,\n",
      "         -3.3977e-02,  6.8966e-02,  5.5082e-02, -1.4348e-02,  1.0123e-02,\n",
      "         -6.9586e-03, -1.8928e-02, -3.8595e-02,  7.0637e-02,  1.6594e-02,\n",
      "          4.0099e-02,  1.1823e-02,  1.0002e-02, -2.6243e-03, -6.5739e-02,\n",
      "         -6.7144e-02, -1.0417e-02,  6.8071e-02, -1.3721e-02, -5.5599e-02,\n",
      "          6.9052e-02,  1.5551e-02, -2.3476e-02, -4.2887e-02,  1.3917e-02,\n",
      "          6.3411e-02, -7.0321e-02, -2.4008e-02, -2.1076e-02, -6.9703e-02,\n",
      "          3.9334e-02,  1.9025e-02, -4.0427e-03,  2.7559e-03, -3.5028e-02,\n",
      "         -6.5715e-02,  2.5836e-02, -4.6632e-03,  2.7409e-02,  5.5346e-02,\n",
      "         -6.7990e-02,  5.9181e-02,  5.1851e-02,  4.7330e-02,  4.0389e-02],\n",
      "        [-6.7480e-03, -8.6381e-03, -4.6758e-03, -4.5668e-02, -1.9818e-02,\n",
      "          6.4190e-02,  6.9930e-02,  6.3735e-02, -6.5352e-02,  3.2960e-02,\n",
      "         -6.1792e-02,  6.5363e-02,  5.3035e-02, -6.9594e-02,  1.4004e-02,\n",
      "         -4.1613e-02, -6.0407e-03, -5.0391e-02,  4.1401e-02, -1.7549e-02,\n",
      "         -6.0390e-02, -5.6454e-03,  2.6692e-02, -3.0175e-02,  3.1639e-02,\n",
      "          3.1301e-02,  3.3278e-02,  7.1613e-04,  3.8975e-02,  6.2811e-02,\n",
      "          7.3780e-03,  5.9974e-02, -1.0318e-02, -5.4778e-02, -5.4925e-02,\n",
      "         -3.9283e-02, -2.7101e-02,  1.4597e-02, -8.3224e-03, -8.5545e-03,\n",
      "          1.3145e-03, -6.3793e-02,  9.2064e-03, -2.2487e-02, -3.8711e-02,\n",
      "          4.4274e-02, -4.5198e-02, -5.2146e-02, -1.9722e-02, -3.1566e-03,\n",
      "          6.4374e-02, -3.3794e-02, -1.7671e-02,  1.0079e-03,  2.9916e-02,\n",
      "         -5.2865e-02,  1.6794e-02, -4.7611e-02, -5.7812e-02, -2.4863e-02,\n",
      "          3.4989e-02,  3.8706e-03,  1.7247e-02, -3.3765e-02, -1.5472e-02,\n",
      "          1.4222e-02, -6.1199e-02,  2.2412e-02, -2.2962e-02,  3.0814e-02,\n",
      "          6.8993e-02, -1.6523e-02,  9.8808e-03, -6.3429e-02,  3.2641e-02,\n",
      "         -2.3346e-03,  1.8872e-02,  5.1447e-02,  6.0915e-02, -1.7272e-02,\n",
      "          2.6221e-02, -3.4509e-03, -2.6610e-02, -5.1863e-02, -4.1230e-02,\n",
      "          6.1108e-02, -1.5349e-03, -2.1992e-02,  3.4367e-02, -2.9099e-03,\n",
      "          1.4499e-02,  1.7564e-02, -6.7521e-02,  1.2467e-03, -2.8478e-02,\n",
      "          4.7494e-02, -2.4400e-02, -5.1284e-02, -2.5828e-02, -6.8766e-02,\n",
      "          3.2686e-02,  6.9519e-02, -1.2648e-03, -3.6621e-02,  2.1485e-02,\n",
      "          4.8003e-03,  6.1523e-02,  2.4813e-02, -8.1129e-03,  5.0403e-02,\n",
      "          5.2981e-02,  2.1440e-02, -1.0474e-02, -5.6044e-03,  2.2974e-02,\n",
      "          4.2686e-02,  6.2809e-02, -3.7588e-02, -5.0048e-02, -1.0602e-02,\n",
      "          4.5451e-02, -3.7596e-02, -2.4368e-02,  2.0073e-02,  3.6329e-02,\n",
      "          1.3395e-02, -5.8243e-02,  5.5343e-02,  3.2568e-02, -3.5433e-02,\n",
      "         -1.6735e-03,  2.7976e-02,  1.6454e-02,  3.1725e-02,  4.9984e-02,\n",
      "         -5.4298e-02, -1.4752e-02, -4.9618e-02, -1.2971e-02, -5.0865e-03,\n",
      "         -6.0458e-02, -7.9551e-03,  5.7786e-02, -3.2106e-02, -1.7834e-02,\n",
      "          6.5837e-02, -3.5728e-02,  2.7162e-02, -4.3608e-02,  1.6266e-02,\n",
      "         -2.2110e-03, -5.5979e-02,  5.9092e-02, -3.2625e-02, -6.0764e-02,\n",
      "         -3.7203e-02,  8.4929e-03, -3.5786e-02,  3.4598e-02,  6.2493e-02,\n",
      "         -1.5061e-03, -1.9153e-02,  2.3253e-02,  5.7481e-02,  4.8324e-02,\n",
      "          1.9751e-02,  3.6052e-02,  4.2621e-02,  5.2280e-02,  6.6904e-02,\n",
      "          2.8683e-02,  1.0379e-02, -1.0413e-02,  2.7859e-02, -1.4771e-02,\n",
      "          1.0640e-02, -5.7859e-02,  4.9119e-02, -3.8189e-02,  6.0461e-02,\n",
      "          2.2393e-02, -3.6557e-04, -5.0069e-03, -6.2893e-02,  6.6756e-02,\n",
      "         -2.8696e-02,  3.8250e-02,  6.6683e-02, -1.1364e-02, -3.6794e-02,\n",
      "          1.1923e-02,  1.8008e-02,  4.5520e-02, -3.4617e-02,  4.0030e-02,\n",
      "          3.6151e-03, -3.4264e-02,  1.7658e-02, -6.4058e-02,  2.6786e-02]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear2.bias | Size: torch.Size([124]) | Values : tensor([-0.0357,  0.0672], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in cnn_.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix model with data/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.crnn import CRNN\n",
    "from utils.utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetImg('data/2/img', 'data/2/label', 512, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=4,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data/char.txt'), 'r', encoding='utf-8') as f:\n",
    "    alphabet = f.read().rstrip()\n",
    "# print(alphabet)\n",
    "converter = strLabelConverter(alphabet, ignore_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(converter.numClass, 100, 0.1)\n",
    "\n",
    "criterion = torch.nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "checkpoint_path = 'pretrain/model_3.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn.conv0.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv0.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm0.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm0.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv1.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv1.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm1.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm1.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv2.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm2.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv3.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv3.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm3.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm3.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv4.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv4.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm4.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm4.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.conv5.weight tensor([[[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan],\n",
      "          [nan, nan, nan],\n",
      "          [nan, nan, nan]]]])\n",
      "cnn.conv5.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm5.weight tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "cnn.batchnorm5.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "linear1.weight tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "linear1.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.weight_ih_l0 tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.weight_hh_l0 tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.bias_ih_l0 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.bias_hh_l0 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.weight_ih_l0_reverse tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.weight_hh_l0_reverse tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "biLSTM.bias_ih_l0_reverse tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "biLSTM.bias_hh_l0_reverse tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "linear2.weight tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "linear2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(iter(dataloader))\n",
    "targets, target_lenghts = converter.encode(labels)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "preds = model(imgs)\n",
    "\n",
    "b, l, c = preds.shape\n",
    "preds_ = preds.permute(1, 0, 2)\n",
    "preds_lengths = torch.full(size=(b,), fill_value=l, dtype=torch.long).to('cpu')\n",
    "\n",
    "loss = criterion(preds_.log_softmax(2), targets, preds_lengths, target_lenghts) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(preds_.log_softmax(2), targets, preds_lengths, target_lenghts) # ctc_loss chỉ dùng với cpu, dùng với gpu phức tạp hơn thì phải\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu ban đầu: [1, 2, 3]\n",
      "Dữ liệu sau khi thay đổi: [1, 2, 3, 4]\n",
      "Dữ liệu gốc cũng thay đổi: [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "class Example:\n",
    "    def __init__(self, list_data):\n",
    "        self.data = list_data\n",
    "    \n",
    "    def modify_data(self, new_item):\n",
    "        self.data.append(new_item)\n",
    "\n",
    "# Tạo một list\n",
    "my_list = [1, 2, 3]\n",
    "\n",
    "# Tạo một đối tượng của class Example, truyền my_list vào\n",
    "example_obj = Example(my_list)\n",
    "\n",
    "# In dữ liệu ban đầu\n",
    "print(\"Dữ liệu ban đầu:\", example_obj.data)\n",
    "\n",
    "# Sử dụng phương thức của class để thay đổi dữ liệu\n",
    "example_obj.modify_data(4)\n",
    "\n",
    "# In dữ liệu sau khi thay đổi\n",
    "print(\"Dữ liệu sau khi thay đổi:\", example_obj.data)\n",
    "print(\"Dữ liệu gốc cũng thay đổi:\", my_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tandat17z",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
